Connecting Streams to Custom Sources: The Spliterator Pattern
Introduction to the Course
Hello, my name is Jose and I would like to welcome you to this course, Steams, Collectors, and Optionals for Data Processing in Java 8. The first module of this course is about Connecting Streams to Custom Sources: The Spliterator Pattern. First of all, what is this course about? The main subject of this course is advanced data processing topics using the Java 8 tools, mainly the Stream API and the Collector API. We are going to see basically three things about streams and two about collectors. The first one is how to connect streams on custom sources, there are many ways to build streams on standard existing sources that are supported in the JDK. I want to show you how to connect streams to your own sources, the sources in your application that are not necessarily supported in the JDK. Then we will see advanced stream patterns, especially from the performance point of view. And we are going to talk in detail about parallel streams, how they are implemented in the JDK and how we can leverage parallel streams to make our applications faster. And then two things about the collectors, first we are going to cover the Collector API from the general point of view. And then we will see how to create custom collectors if the existing collectors available in the JDK do not fit exactly our applications needs.

What Are You Going to Learn in This Course?
So what are you going to learn in this course? Well you are going to learn how to connect the existing Stream API to your own non-standard particular sources of data you may have in your application. Then you are going to learn how to use and to leverage the advanced flatMap operations from the Stream API and then we will also talk about streams of numbers added to the JDK for performance reasons. One of the very interesting point of the Stream API is that it can go parallel just by a simple method call, we will see this point in detail and how to efficiently use parallel stream in your application. Then you are going to learn how to master the concept of Optionals from the JDK. This concept is a new concept introduced in Java 8. It enabled the development of error list data processing streams and we are going to cover this point in detail. And you will learn how to use existing collectors available in the JDK, some of them are quite complex, and how to create our own collectors. Let us quickly browse through the agenda of this course. First of all, we will see the spliterator pattern, this is the subject of this first module. The spliterator pattern enables the connection of the stream API to our custom sources of data. Then we will see how to use the flatMap operation on the streams and the stream of numbers. The third module is about parallel streams with many details on how parallel streams are implemented and how to use them. The fourth module is about Optionals and we are going to see how we can build data processing pipelines built on Optionals. The fifth module is about the Collector API and the standard collectors available in the JDK. And the last module is about how to build custom collectors for our applications (Loading).

What Should You Know to Follow This Course? Agenda of This Module
One last point before jumping into the first module, who are you and what do you need to know to follow this course? Well, of course, this is a Java course, so you need a good knowledge of the language and a basic knowledge of the main APIs of Java, basically the object class, the string class, and these kind of things. You also need to have some knowledge of the collection framework, the Collection API, because basically streams are built on top of the concepts of the Collection API. And you also need to have fair knowledge of generics. Now generics is a very complex topic in Java, you do not need to know everything about generics, basically you need to understand how generics are used in the collection framework. And you also need to know about lambda expressions from Java 8 and basic knowledge of the Stream API. This course is not an introduction to lambda expression, nor it is an introduction to the Stream API, if you need an introduction to those topics, you can check my other course on this called From Collection to Stream in Java 8 using Lambda Expressions. It is available here on Pluralsight and it will explain you everything about lambda expressions and the Stream API, everything you need to know to follow this course. So let us talk now about the agenda of this module. First of all, we are going to see what is a spliterator? A spliterator is basically an interface, it is not very complicated, but we still need to go through this part to understand how to connect streams to nonstandard sources of data. Then we will build, of course, our own spliterator and see how the stream API works on our own spliterator. And we will use this spliterator to regroup the lines of a text file. This will be our use case.

What Is a Spliterator?
And now with no further ado, let us answer this question, what is a spliterator? What if I want to connect a stream on the nonstandard source, on a custom source, that is really particular to my application? Well the solution is called the spliterator. What is a spliterator? It is a special object, also new to Java 8, on which a stream is built. Let us see that in this part. What is a spliterator? Let us check for that, the Collection. stream method. This method is in the collection interface, of course, it is a default method. It calls StreamSupport. stream and provides a spliterator as a parameter. This spliterator is built by calling the spliterator method from the collection interface. Let us have a closer look at this method. This method in fact is the following, it's a call to spliterators. spliterator. I pass this as a parameter, this is the collection I am in. And we can see that the returned stream from the stream called is in fact built on this spliterator. This spliterator is a new interface from Java 8, we are going to see this interface in detail later in this part, and this object spliterator models the access to the data for a Stream. This spliterator can be seen, in fact, as the iterator of the collection. The collection access it's data through the iterator object and the stream does the same through the spliterator object. We can have a look at the spliterator's for ArrayList and HashSet, the well known implementations of collection and list for ArrayList and set for HashSet. The spliterator of ArrayList is an ArrayListSpliterator and the spliterator for the HashSet is the HashMap. KeySpliterator and both take the ArrayList or the map object as a parameter as well as some integers that hold 0 or -1 values. So a Stream is in fact divided in two things, first an object to access the data and this object is the spliterator. This spliterator object has been made to be customized. This interface has been made simple to be implemented and easily implemented in my application. And another object to handle the processing of the data, which is the ReferencedPipeline. ReferencedPipeline is in fact the implementation of the stream interface that holds all the algorithm and especially the map/filter/reduce algorithm. This object, I should not touch it, and it has not been done to be overridden or modified in any way.

The Spliterator Interface, What Do We Need to Implement?
Can we build our own spliterator? Well the answer is yes. This interface has been designed to be implemented and once I have implemented it, I can provide this custom implementation to the Stream API and use the complex algorithm of the ReferencedPipeline to process my data. Let us examine this interface. I have in fact four abstract methods called tryAdvance, trySplit, estimateSize, and characteristics. We can see that just by the name of those methods the spliterator is indeed much different from an iterator. An iterator has basically two methods, next and hasNext and a third method called remove. But it does not have a tryAdvanced or estimateSize method. Why are those method named like that? Just because the spliterator holds a logic to access the source of a stream and a source of a stream may be inbounded, may be infinite, so I might not be sure if I still have object to consume in it. We can see just on this simple interface that the concept of stream is much, much different than the concept of collection. We also have three main default methods, in fact we have a bit more. For each remaining, getExactSizeIfKnown, which our default method based on the tryAdvance on the characteristics and on the estimateSize abstract methods. And we also have a method, hasCharacteristics, also based on the characteristics method I need to provide. These methods could be overridden, but I do not need to do that and in our own implementation of the spliterator most of the time it is not done, we just keep those methods as they are. So basically creating a spliterator looks quite simple since we just have four methods to implement. We also have a set of very interesting constants defined in the spliterator interface, here is the list, ordered, distinct, sorted, sized, nonnull, immutable, concurrent, and subsized. And we can see that all those constants are in fact bits in a special word and they will be mixed in a single word of bits named characteristics.

The ArrayListSpliterator: the tryAdvance() Method
The JDK itself is a good source of inspiration, so let us have a look at how the spliterator interface has been implemented for ArrayList. ArrayList is probably the simplest implementation of the list, so the spliterator should be simple too. In fact, the ArrayList spliterator is built on the list itself on an index that just points to the current elements being consumed and on a second index called fence that will point to the end of the list. We are not going to talk about expectedModCount for now. How does the estimateSize work? Well it just returns getFence -index, that is the number of element to be consumed in that list, really easy to understand. How does the tryAdvance works? It takes a consumer as a parameter and should pass the current element to that consumer. So basically it checks if I still have elements to be consumed, so I compare the current index to the current fence. If I still have an element to be consumed I just advanced by 1 index = i + 1. Get the element by querying the list this spliterator is built on and just pass that element to the accept method of the consumer passed as a parameter. Now don't worry, if you check yourself the tryAdvance method in the ArrayList spliterator, you will see that it is a bit more complex than this one, I have just simplified it for readability purposes.

The ArrayListSpliterator: the trySplit() Method
And now let us have a look to the trySplit method. What does it do? In fact, it takes the fence of this ArrayList spliterator, now remember the fence points to the end of this ArrayList, it takes the current index, divide the number of remaining elements by 2 with the mid index and the with a little check returns a new spliterator, built on the same list, on the lo index, which is the current index, and on the mid index, which is half of the remaining elements. So basically this trySplit method returns another spliterator built on the half of the remaining elements and more precisely on the first half of the remaining elements. Now why should I do that? I would do that only if I am processing my data in parallel and indeed the Stream API has been made to process data in parallel, we are going to see that in detail in the remaining of this course and this is the trySplit method that we'll help us doing that. Now if I do not want to process my data in parallel I can return a dummy spliterator in my trySplit method and it will even simplify the implementation of this spliterator. So implementing a spliterator requires a good knowledge of the underlying source of data. I haven't shown you the spliterator of the HashSet, it is much more complex than the spliterator of the ArrayList. Why so? Just because the way the data is recorded in an HashSet is more complex than a simply array of elements and if I have a complex source of data, my spliterator will be probably complex too. But really, implementing a spliterator is in fact quite simple, I just need to understand correctly what the three methods I need to implement are doing.

Introduction to the Live Coding: How to Write a Spliterator
Okay so now is the time for a little live coding session. What are we going to see in this session? Well we are going to take the example of a text file with people written in it and we are going to write some code to analyze this text file and create a stream of people out of this text file. Now the trick is that each person is written on more than one line and if you want to do that, only with the Stream API, we will have to build our own spliterator to do that. So let us jump in this live coding session. So let us now create our own spliterator on a nonstandard source of data. What do we have here? Well, first of all, we have a Bean Person, which we are going to use, very classical and very standard JavaBean with three properties, name, age, and city, the custom constructor to easily build our people, and a toString method to display them properly on the console. So this is a very standard JavaBean called person. On the other hand, we have a text file with people in it. Each people is returned on three lines of this text file. We have Alice, age is 52, and she lives in New York, and for instance, Erica, age is 32, and she lives in Berlin. And we would like to read this file and convert this file into a stream of instances of person (Loading).

Using Our Own Spliterator: Setting up the Application
Now what do we have as a standard tool to redefine into a stream? Well we have a very and the API, first we need to build a path on this file using the static method get from the path's class, here the path is files/people. txt, and from this path we can leverage the files. lines method by just passing the path as a parameter. This method directly returns the stream, which are in fact the lines of this file. Of course, it also throws exceptions, so we can wrap that in a try with resources statement here and catch the IOException. I don't really know what to do with it, so let us just print the StackTrace in case this exception is raised. Of course it won't be in the case of our example. And now what can I do with this lines stream object? Basically this object is a stream of string, so every element of that stream is in fact a line of this text file. So the first line will be the name, then the age, then the city. Then at the end of the third line I should create my person instance, put it somewhere, probably in a list or an array, and then do it again for the next three lines. So what I can see is that this object will read my file line by line where what I need is an object that reads my file three lines by three lines. So this object in fact is not completely adapted to my problem.

Using Our Own Spliterator: Writing the Use Case Pattern
So the right way to address this problem is in fact to create another spliterator in this stream that will do the job. First we need to extract the spliterator from the stream. So lines. spliterator, the pattern is exactly the same as the one I have to extract the iterator from a collection, put that in a variable. This is my lineSpliterator and from that spliterator I can build another one and this is exactly the topic of this live coding session. Build another one that will be a spliterator of person. So let us do that. Let us just copy paste this. I want a spliterator of person, let us call it peopleSpliterator, and for the moment I will keep it null because we are going to see into details how to build this spliterator. From this spliterator I should be able to create a stream of people simply by calling StreamSupport. stream, passing this peopleSpliterator as a parameter and false, which tells the API that I'm not going to go parallel with that stream, and put the results in a stream. This is a stream of people built on the stream of lines. And what I can do with the stream of people, for instance, people. forEach(System. out::println) will print all the elements of that stream. Now of course if I run this code I will get a null pointer exception. What I need to do is to provide an implementation of this spliterator of person.

Implementing Our Own Spliterator: The First Methods
We are going to call this spliterator of person PersonSpliterator and we are going to pass the LineSpliterator as a parameter to the constructor of this new class. Let us add the new here. Let us create this class with the right constructor. So PersonSpliterator implements spliterator of person. I have the right constructor here, it is a public class of course. Let us ask our IDE to implement abstract method, so I have tryAdvance, trySplit, estimateSize, and characteristics just as we saw in the slides. Of course, this LineSpliterator should set a private field of my class that I can create just like that. Okay, now let us take a closer look at those four methods we need to implement. The first one we are going to implement is characteristics. Why? Because we are just going to return the characteristics of the spliterator we are built on. So just here, return lineSpliterator. characteristics. We are not going to change the characteristics of this underlying spliterator. The second one, fairly easy to implement too, is estimateSize. If the size of the underlying spliterator is, for instance, 18, what does it mean for the number of people we have in it? Well, just take the example, we know that each person takes three lines in the text files, so basically we need to divide the number of elements of the underlying spliterator by 3 to compute the number of people we have in this file. So here we can just return this lineSpliterator. estimateSize. This is a spliterator, so it also has an estimate size method and here we are just going to divide this estimateSize by 3 to get the number of people. The trySplit method, as we saw it, is just there in case we want to go parallel. This we are not going to do it in that case, so we can just return null. Don't forget in the main method to pass false here when we build this stream instance so that the implementation is not going to use this trySplit method. Okay, so this one returns null. And now the last one we have to implement is the tryAdvance method.

Implementing Our Own Spliterator: The tryAdvance() Method
Now we need to understand exactly how this tryAdvance method works because it does not work as the next method, for instance, from the iterator interface. Here, the tryAdvance method is called by the Stream API itself. The Stream API passes an action object, which is a consumer, and what we are going to do with this action object, in fact, we should call its accept method passing it the current element of that stream of the stream we are currently building. So what do I want to do with this action consumer here which basically is a consumer of person? This imposed by the signature of the tryAdvance method where I should build a person instance by reading three lines of the underlying stream accessed by the underlying spliterator and called action with the new person just built. So let us just do that. How can I consume object from this lineSpliterator? Well just like that, by calling the tryAdvance method of this lineSpliterator. So first of all we are going to consume the current element of this lineSpliterator. What do I take as an argument here? I take a consumer of line, so line, and what is this line? This line is the name of the person since it is the first line of this file. So let us do that. I am just going to all this name = line here. Create this private field name. My IDE can help me here. This is a consumer. A consumer is a lambda expression that takes an argument of type line here, since I am working with the lineSpliterator, and that does not return anything, and indeed this _____ does not return anything. Now I need to do it with the second line and the second line is the age of the person I am reading. So I need to convert this string of character into an int so I need to pass this int, once again create this field with my IDE and the last line I need to read is the city where this person is living. Let us create that field. Here it is. Now I can create a new instance of person, p = new Person of name, age, and city. And this new instance is the instance I'm going to pass to the accept method of this action object, so action. accept(p). And this is going to build a special stream of people by reading lines 3 x 3 from the underlying stream of line accessed through this lineSpliterator.

Implementing the tryAdvance() Method: Setting the Returned Object
Now it's not quite over at that point. Why? Because this tryAdvance method has to return a Boolean. This Boolean should be true as long as I have elements to produce and should be false once I'm done. And this is the same for this tryAdvance method I am invoking here. So basically if this tryAdvance method returns false at some point, it means that I have consumed all the elements of the underlying stream. So I need to check if this tryAdvance is true or not, combine all those three calls in one Boolean, that will tell me if I am done with reading this file or not. So let us do that. Let us wrap that in an if statement. So if I can advance one line, then advance the second line and then advance the third line, it means that I can create a person and pass it to the accept method and from here return true because this person is valid and I might have more person in my stream. If this is not the case, if one of these calls returns false, then I should return false because I am done reading my underlying text file and this is basically all I need to do to create a stream over a stream, that is, to connect a stream on another stream by transforming it using this spliterator pattern.

Implementing Our Own Spliterator: Running the Example
You can see that creating a spliterator in fact is not that complicated. I have two methods that are quite easy and most of the time very easy to implement, but read the underlying spliterator. This method trySplit may be left blank, may be unimplemented, if I do not want to go parallel I do not need the trySplit, so all the complexity, all the complex logic, is in fact in the tryAdvance method. And now we can build our peopleSpliterator by just calling this constructor and the rest of our code should work as we expect. So let us run that. Run file. And indeed we can see that all our people have been read by our spliterator. We could pass from a stream of lines to a stream of people by regrouping the line of the underlying text file 3 x 3.

Live Coding Session Wrap-up
Now is the time to wrap up this live coding session. What did we see? Well we saw how to create a complex spliterator from a given spliterator. Now when we have a stream it is very easy to get the spliterator from that stream, so it is very easy to get a standard spliterator from a standard stream built from a standard API from the JDK and once we have that we can basically write any kind of spliterator on this given spliterator. So this is very useful to convert streams of data of one type to another and here we used it to process a text file by regrouping the lines of this text file in a very clean and very easy to read way here 3 lines by 3 lines. We can, of course, extend this example to any kind of data regrouping from one stream to another.

Module Wrap-up
Time now to wrap up this module. What did we see in this module? Well, we saw advanced patterns to build custom streams on nonconventional sources of data. The example we took was building a stream on another stream, of course, we can imagine any kind of situation. This is built on the use of the spliterator pattern. The spliterator is the object the stream API uses to access its data. So we saw how to create our own spliterators to built streams in a nonstandard way and this proved extremely powerful and extremely useful too. The use case we saw in the live coding session was about reading a text file by regrouping its lines. Why did we have to do that? Simply because our atoms of data was spread on three lines instead of one. So it was extremely useful to do that and to create a stream of people out of a stream of lines on a text file. This is the end of this first module. Thank you for watching and bear with me for the second module about advanced streams patterns and streams of numbers.

Advanced Java 8 Stream Patterns: FlatMap, Streams of Numbers
Introduction and Agenda of the module
Hello. My name is Jose. Welcome to the second module of this course. This module is about Advanced Java 8 Stream Patterns: FlatMap and Streams of Numbers. What are we going to see in this module? Let us quickly browse through the agenda. Well, first of all, we are going to see how to concatenate streams, how to merge the data from several streams in one big stream. There are several patterns for that, some are more efficient than others, and this is what we are going to see. Second, we will see a very special topic called the state of a stream. We already know that a stream doesn't hold any data, streams merely connect itself to a source of data and pulls that data from this source, consumes that data from the source. So there is basically nothing in a stream. But there is a still a state in a stream. We saw a hint of that in the previous module with the characteristics method. We didn't explain it in the previous module. Now it's time to do it and this is the second topic of this module. And then the third one will be about specialized streams, streams of numbers of int double and long taken as primitive type from the Java language. This part is all about performances because those specialized streams have been introduced for performance reasons.

Concatenating Streams: The Stream.concat Pattern
Now let us see how to concatenate streams and we are going to see two things in this part, the concatenation of streams and the notion of FlatMap. So what are the use case we are going to study in this part? Suppose we have two text files, text1 and text2. txt. We know from a previous part how it is possible to open a stream on the lines of those two text files and what we would like to do is to create a stream with all the words of those two text files. So we can create a path on the first file, use the try with resource pattern from Java 7 because a stream is an autocloseable object, so the first stream of string will be the lines of the first text files by calling Files. lines and by passing the file as a parameter. We can do that for our two text files and then we have two streams, s1 and s2, with all the lines of our two files. We can then concatenate those two streams by calling the concat static method from the stream interface. Each will return a stream called s10 with all the elements of the first one and of the second one in it. The concat method takes only two arguments, so if I want to concatenate more than two streams I need to write this kind of code and might run into errors like StackOverflows or things like that. The documentation of the Stream. concat method is very precise. What does it say? It says that the elements of the first stream are followed by all the elements of the second stream. So there is a huge constraint in this concatenation. It is not the mixing of the two elements, but it is really the first elements and then the second elements. The order of the elements is preserved and it has a cost, so this Stream. concat method is more costly than it seems, and more than that, we will see in the next part that it has problem with parallelization. So if we do not need to preserve the order of the element, then we should try to use another pattern that we are going to see now.

Merging Streams: The Stream.of and flatMap Pattern
The other better solution to merge stream is following. In fact, instead of concatenating s1 and s2 and then the result of that concatenation with s3, we can build a stream like that, Stream. of(s1, s2, s3) and since this of method takes a varargs of an argument, I can pass as many streams as I need to it. Now the problem is what is the object returned by this Stream. of method? The answer lies in the type of the object I pass to the of method. These three objects are streams of strings of characters. So the returned object by this of method is a stream of stream of string because this is the type of s1, s2, and s3. So how many elements do I have in s? I have three elements, which are s1, s2, and s3. This is not quite I want to do and this is certainly not what I call stream concatenation. So far I have built a streamOfStreams so the job is not really done. But I have a solution to that problem because I have a FlatMap method on the stream interface. What does this method do? It flattens the streamOfStream. The FlatMap operation is a special operation that takes a function so it is just like a regular mapping, a regular mapping takes a function too. But this function has to return a stream. The signature of this function is bounded and the returned object should be a stream of something. And a special case of that is this function can be an identity function. Takes a stream of strings, returns a stream of strings, and in fact the implementation is extremely easy. It just takes a stream and returns the exact same stream. So it does not do much.

How Does the flatMap Pattern Work: Examples
Of course, this function, flat mapper, can be used as a regular mapper passed to the map operation of the stream API, but it will be much more useful to pass it to the flatMap method. And then what is going to happen, the resulting the stream will be flattened. That is, I go from a streamOfStreams and if I apply this flatMap operation, all the resulting streams will be flattened into one stream of strings. So this is exactly what I want to do and this time I do not have the overhead I had using the concat operation. This is great. This stream has all the elements I need in it. So basically, how can I write this pattern? Stream. of(s1, s2, s3) as many streams of string I need. Then I call the flatMap operation, just with a special flat mapper, which is in fact an identity function, and I have a static method especially for that in the function interface and it will return me exactly what I want, a stream of strings with all the elements from the concatenated streams of strings.

Splitting a Text into Words Using the flatMap Pattern
Of course I've got many other flatMap use cases. In this first example we could merge the lines of those two files into one stream. So now could we split those lines into words and merge them into one streamOfWords? That is, going from a streamOfLines to one streamOfWords? Well we can write a function that splits a line into words into a streamOfWords and then apply this function to a flatMap of that stream and that should do exactly what we want. Let us see that. We can build a first version of this function, taking line and returning Stream. of line. split and passing just a regular expression here, a blank space to the split method. This split call will cut our line into an array of words, an array of strings of characters, passing this array to the Stream. of method will build exactly what we need, that is Stream. of word for each lined passed as a parameter. But we can do better and much more elegant with the Pattern. compile object. This object is in fact a pattern object from the Java regular expression package. We just pass a regular expression in a string of characters to the static compile method of the pattern class and this regular expression will be used to split the line in pieces here in words. And on that object, I have a very handy method, splitAsStream, it returns a stream of string, each element of that stream is in fact a word of that line. So once we've done that we can use this function as a flat mapper and pass it to the flatMap method. Let us build our stream on s1, s2, s3, et cetera. Let us flatMap these stream using the Function. identity. This will return a stream with all the lines of all the files I have analyzed and then I can flatMap into words to get a stream with all the words contained in all the files I have put as arguments of the of method. What is the return type of those two calls? Well the return type is simply a streamOfWords and if I want to zoom in each step of this processing, the first call returns a streamOfStreams of lines, the second call returns a streamOfLines, it is a flat mapper. It is the flat mapper we just saw. And the third call returns a streamOfWords.

Splitting a Text into Words: Putting the Result in a Set
Now written as is, this processing might not be that useful. Why? Because it returns a streamOfWords and this streamOfWords is basically empty, there is no data in it. If I just write this code, since the flatMap method returns a stream, no data will be processed. So I need to process it a little further and thus giving a hint on the following of this course, we can add a collect call and pass a Collectors. toSet object as a parameter to this collect call. What will it return? Well quite magically it will return the set of a string that I'm going to call words and this set will hold all the words of all the files that I have added to the off call on the first line of this processing. So this is a very powerful way of extracting words from more than one text file, just four lines of Java code of the stream API and I have a set of string with no doubles in it, of course, all the doubles have been removed since this is a set, and it is also extremely efficient to compute.

State of a Stream: The Stream.characteristics() Method
Let us now talk about the state of a stream and we are going to talk about two methods, the sorted method and the distinct method. A stream has a state, we said and we defined a stream as an object with no data in it. But when we said no data in it, we were talking about the data held in the source and consumed by the stream. But it does not mean that there is strictly nothing in a stream and in fact there is one important thing, which is the state of the stream that we are going to see now. This is the implementation of the stream method from the ArrayList class, we already saw that when we were building the spliterator. Remember this is the spliterator method redefined in the ArrayList class, the one that builds an ArrayListSpliterator. So we already said that the spliterator interface describes how the stream should access the data of the source and this is the interface we want to implement if we need to consume data from a custom source and we saw an example of that. Let us have a look at the only method we did not show, the characteristics method of this implementation. This is the characteristics method from the ArrayListSpliterator. We can see that it returns three constants from the pool of constants we showed in a previous slide. Spliterator ordered, sized, or subsized. Now, ordered, sized, and subsized are in fact special bits of a special word inside the spliterator, which is precisely called characteristics and these 3 bits describe in fact the state of a stream. This is the method for ArrayList. It is interesting to compare it with the method from HashSet. And we can see that an HashSet is not ordered, might be sized if I know the size of the map, let us not go into much details here, and is distinct whereas the spliterator of an ArrayList is not distinct.

What Are the Eight Characteristics of a Stream?
Here are all the elements of the state of the stream, let us quickly go through them. The ordered bits tells us that the order matters and this is the case indeed for ArrayList. It is not the case for HashSet for HashSet has no order. The distinct bit tells us that there is no duplication in this spliterator and indeed it is the case for a set and certainly not the case for an ArrayList or for a list generally speaking. We have a sorted bit, which would tell us that the elements have been sorted using comparable or a custom comparator. This is not the case for a set, not the case for ArrayList, but it should be the case for sorted set. Sized means that the size of that spliterator is known and here we can see that we need that because a stream may be unbounded, so if I build a stream on a list I will know the amount of elements in that list, but if I build a stream on the circuit, for instance, I cannot foresee the elements generated by that circuit. Nonnull will tell me that I do not have any null values in my stream. Immutable that my stream is indeed immutable. Concurrent, it means that my stream is built on the concurrent aware structure, it's not the case for ArrayList, not the case for HashSet, it might be the case for other structure, and I indeed need concurrent aware structures if I want to go parallel. And subsized, that looks like the size bit, meaning also that the size is known. So now that we've been through these characteristics, what can be done with them and first of all, can we change them?

How Does the Steam Methods Modify the State of a Stream
And indeed some methods, some calls on the stream, will change the values of those bits. Either implicitly, which is the case for map and filter. Either explicitly, which is the case for sorted or distinct. Let us examine those cases. A filter call will set the sized bit to 0. Why? Because I cannot tell how many people are younger than 20 before trying to process all the data. So a filtering call puts the sized bit to 0. A map call will remove the distinct and the sorted nature of a stream and this is quite easy to understand. Suppose I have a sorted set of people, if I build a stream on it, this stream will have no doubles and will be sorted, so both bits will be set to 1, and then I map it to the ages of those people. I certainly cannot tell if I won't have any doubles in the ages of those people and if the returned streamOfAges will also be sorted as was the streamOfPeople. So the distinct and sorted bits have to be set to 0. And this is the same case for the flatMap operation. The size of a stream is not changed by the mapping operation, but the flat mapping operation can change it. Suppose I call a flatMap on the stream of two streams and one of the streams is unbounded, is infinite, then the size bit should be set to 0. The sorted and distinct call are a bit particular since they are just there to set certain bits to 1, namely sorted and ordered for the sorted call and distinct for the distinct call. So in fact, calling this method is very lightweight, it is just a matter of setting your special bit to 1. And the same goes for the unordered method, the unordered call, in fact, is just there to set the ordered bit to 0.

State of a Stream: A First Example
Right, so why do we have all these? Why has it been introduced on the stream interface? When in fact, it is all about performance. Suppose we have this processing, we take a list of people, build a stream on that called distinct. Distinct returns a stream so it is just an intermediate method, it does not trigger any computation. Then we have a sorted call, same as distinct intermediate method, and then a forEach, which is indeed a terminal operation, that will trigger the processing of the data. Now suppose that my collection of people is in fact a HashSet. The runtime will know that, so the stream built on that HashSet will have a distinct bit already set to 1, so the distinct call does not do anything and it will not trigger any kind of processing on the forEach call. This is not the case for the sorted call. This sorted call will set the sorted bit to 1 and by doing that it will trigger a quick sort when the forEach method will be called. Now suppose that our collection of people is in fact a sorted set. A sorted set is already sorted, so I certainly do not want to call quicksort again and this is exactly what happens. The distinct call does not modify anything since the distinct bit is already set to 1 and the same goes for the sorted call. So in fact, the forEach method call will not trigger any kind of processing by the distinct or the sorted call.

How to Use the Stream.sorted() Method with a Comparator
Let us make a side remark on the sorted method. The sorted method can be called on the stream of comparable objects. The problem is that it is not verified at compile time, it is only verified at run time. So if my stream is not a stream of comparable objects, my sorted call will generate a exception. I need to be careful about that. I can also provide a comparator as a parameter to the sorted method. In that case this comparator will be used to compare the objects of the stream and of course if the objects. are not comparable, the processing will not raise any kind of exception.

Live Coding Session: Presentation of the Example
Now is the time for our first live coding session. What are we going to see here? Well, we are going to merge a large amount of text and cut this text into words using the flatMap operation. We are going to work on large text, which is the text of the Tom Sawyer novel by Mark Twain. So what do we have here? We are going to work on a large text file containing the complete text of the novel by Mark Twain called The Adventures of Tom Sawyer. This text file is about 9000 lines long. You can download it directly through this URL. By the way, this page is the page of the data sets used by Robert Sedgewick for his books on computer science and algorithms. What did I do with this text file? Well, I split it into four smaller files and wrote the code to read those four files into four streams of strings being the streams of the lines of those text files. This is what these Files. lines patterns do, it returns a streamOfLines when opened on a text file. We can just display the number of lines we have in each of those files, let us run this code. We have roughly 2000 lines for the 2 first streams, 3000 and 1000 for the 2 last streams.

Live Coding: Merging Streams Using Stream.of and flatMap
So what do we want to do first? Well we want to merge those four streams into one and for that we are going to use the Stream. of pattern by passing stream1, stream2, stream3, and stream4 as a parameter. The of method takes a vararg as an argument. So this is an array-like structure and it will return a stream of the elements passed as parameter. Let us just put this into a variable. This is a streamOfStreams and if I want to check that I can just display the number of streams in this streamOfStreams by calling, for instance, the count method. Let us run this code and indeed I have four elements in this streamOfStreams, which is exactly that, stream1, stream2, stream3, and stream4. Let us comment out this code, this one also, by the way. So what I can do first with this streamOfStreams is flatten it in a stream of strings. We saw the patterns in the slide, streamOfStreams. flatMap and then I need to pass a special function as a parameter, that takes the current element of this stream, that is a stream of string, let us call it simply stream, and that will return something in a stream. So this special function has to return a stream and in fact, it can return the same stream, so like the identity function and what will it do? Well it just take the content of those four streams and flatten it, that is extracting it and putting it in one big stream with all the elements of those four streams in it. So if I put this into a variable, indeed it will be a stream of string with all the elements of those four streams in it. Let us call this stream simply streamOfLines because it is exactly this that we have in this stream, let us display the number of lines we have just by counting them, streamOfLines. count. Run this code and indeed we have roughly 8500 lines in those 4 streams. Now we can also write this function in another way, just like that, Function. identity. Identity is a static method from the function interface that does exactly what it tells, that is take an element and returns the same element.

Live Coding: Splitting a Large Text in Words Using flatMap
Okay great, now from this streamOfLines, what I would like to do is build a streamOfWords. So let us comment out this code. How can I do that? To create a streamOfWords out of this streamOfLines I need to build a function that takes a string as a parameter and that will return a stream of string as a returned element. Let us call this function lineSplitter. What is this string it takes as a parameter? Well it is just a line from this stream of calls and this stream of string here will be in fact a streamOfWords, words obtained by cutting this line according to the blanks. We have a special pattern to do that, build on the pattern object. The pattern object has a compile static method, I can pass a regular expression to this compile method that will be used to cut out this line object, and this pattern object has a very handy method splitAsStream, which will return exactly what we need. We pass the string of character we want to get and this does exactly what we need. We take a line and return the streamOfWords obtained by cutting this line using this regular expression. So now we can take this streamOfLines and flatMap it once again using this lineSplitter. This lineSplitter returns a stream of string so it can be used as a flat mapper and then from that we will have a stream of string, which is this time a streamOfWords. How many elements do I have in this streamOfWord? Let us see that (Typing) StreamOfWords. count, let us run this code. We have roughly 70, 000 of words in this novel. Now we can do a little better because obviously those are all the words without removing the doubles, without removing the word that I'll spell differently according to the case. So let us take this streamOfWords and let us go a little further. We can, for instance, map by taking a word and putting it toLowerCase. We can call the distinct method to remove the doubles and let us run this code again. Now we have about 12, 000 different words in a novel by Mark Twain. And we can go one step further. We can, for instance, take only the words of four letters, word. lengths = 4, let us run this code. We have about 1000 words of length 4 letters. So once we have this stream, we have many possibilities to play with this text and you can see that we end up with a very simple and very clean pattern to merge a text file and to play with the lines and with the words.

Live Coding Session Summary
Let us do a little wrap up on this live coding session. What did we see here? Well we saw how to merge streams together using the Stream. of and the flatMap patterns. Using those two operations allow us to merge any number of streams with a very efficient and very powerful pattern. And then we further played with the flatMap pattern and we efficiently split our streams, which were streamsOfLines, into StreamOfWords. We played with a fairly large amount of data, the text of a full book, about 10, 000 lines of data and the pattern was still very fast and very efficient.

Streams of Numbers: Converting from a Stream of Objects
Let us talk now about the special streams, streams of numbers. And I would like to see two things here, first why have they been introduced? And second, what method they provide that are not on the regular stream interface? First of all, what is a stream of numbers? Let us take an example, basically we would like to compute this time the average of the ages of our list of people and what I would like to write as a code is the following, build a stream on my list of people, call the map method to extract the ages of those people, then filter out all the people younger than 20 and call an average method. Now the problem is there is no average method on the stream of T on the interface stream of T. And it is quite normal. What would it mean to compute the average on strings of characters, on people, on whatever? But it does exist, on this specialized IntStream built on the primitive type int defined in Java. So what I need to do, in fact, is to convert this stream of integer and after this map call I have indeed a stream of integer. How can I convert that stream of integer into an IntStream? It should not be that complicated.

Optimizing the Conversion from a Stream of Objects
And indeed it is not complicated at all. I have a mapToInt method, I can provide an identity function to this call. This identity function will use auto-unboxing and auto conversion to build an IntStream and this time I can call the average method on this IntStream. But we can do ever better. in fact, if I look closely, I am already having a stream of integers in my first map call, so why not call mapToInt at the first step of my stream and by providing the same method, person. getAge because person. getAge is in fact an int, it is not an integer. And indeed this will work, this mapToInt call returns an IntStream instead of a stream of integer and calling it at that place will lead to a much more efficient processing. Why? Because I will not have to pay the price of boxing and unboxing my integer over and over to compute all the filtering predicates.

Patterns to Build Streams of Numbers
So stream of numbers are just there to avoid the cost of boxing and unboxing integers, longs and doubles into intLong primitive type and double primitive type. It is in fact mostly a matter of performance to use streams of number. I have three types of such stream, the IntStream, the LongStream, and the DoubleStream. The patterns to build them are extremely simple. I can build a streamOfLong by calling the of method on the LongStream interface. I can convert a stream of integer into an IntStream by calling a mapToInt method and I also have mapToLong and mapToDouble method. I can also do the contrary, box the stream of number if I need it and we are going to see examples where we need to do this kind of thing, so the boxed method is just there to return a streamOfLong in the case of a long stream. If I do not want to call this boxed method, I have also a mapToObj call on the LongStream, IntStream, and DoubleStream that will return me a stream of object following the type of function that is passed to this mapToObj method.

Specialized Methods: min(), max() and summaryStatistics()
Stream of numbers have special methods that are not defined on stream of T just because a T may not be a number and most of the time it is not a number and there are methods that are only defined on numbers. This is the case of the sum method, the case of the min and the max method, and the case of the average method. Note that since the min, the max, and the average have no identity element, the result is wrapped in a special type of optional as I have IntStream, DoubleStream, and LongStream, I also have OptionalInt, OptionalLong, and OptionalDouble. Basically an OptionalInt works in the same way as an Optional of integer. The methods on that object and their return type are just different, they return primitive types instead of objects. And we also have this very interesting method, summaryStatistics. What does this method do? In fact, on one pass over the data it computes the sum, the min, the max, the count, and since it has the sum and the count it can also compute the average, just on one pass over the data, so it is a very efficient way of extracting basic statistics over a set of data. We can just have a look at our simple implementation of this IntSummaryStatistics object is. In fact, it is just a simple consumer. In our case it is an int consumer, but I could imagine it any type of consumer. It has an accept method that just update all the statistics computed over the data in one pass. So if I need to have other types of statistics on my set of data, it is very easy to build another summaryStatistics like object to get some inspiration from this class of the JDK and to build my own to have those statistics in a very efficient way. By the way, there is also another method from that class that will deal with the computations in parallel, so yes this computation can also work in parallel.

Live Coding Session: Introduction of the Scrabble Example
Time for a second quick live coding session. What do I want to show you here? Well I want to play with you with streams of int, primitive type that are IntStream, and we are going to take a very well known example with the game of Scrabble. So what do we have for the second live coding session? Well first we have a file, ospd. txt, with all the words allowed at Scrabble. Yes this is the Scrabble game, the well known Scrabble game, in its English version. We have roughly 80, 000 of those words. And we have a second file containing all the words used by Shakespeare in his books, novels, and theatre, and we have about 30, 000 search words. Those two files have been taken at those URL, once again it is the same page, the data set page used by Robert Sedgewick in his books about computer science and algorithm. What are we going to do with these files? Well, first we are going to read them with the pattern we already know. Map those works into lower case and collect them in sets. We haven't seen the collector pattern yet. We will see it in much detail at the end of this course. And then we can just print out the number of words by Shakespeare and the number of words at Scrabble. Let us run this code. We have roughly 30, 000 words by Shakespeare and 80, 000 words in the Scrabble dictionary.

Live Coding: Computing the Score of a Word in Scrabble
What do we want to do? Well we want to compute the best word by Shakespeare if Shakespeare had been a Scrabble player. To do that we need on more information, which is this one I just copy/paste this array here, which gives the score of each letter at the English Scrabble. So we know that the a is worth 1 point, the f 4 points, the q 10 points, and the z 10 points too. The first thing we are going to do is to build a function to compute the score of a given word. And this function will rely on IntStream and this is why we are taking this example. So how does this function work? It's a function that takes a word as a parameter and that return an integer. Let us call it score. It's a regular Java util function. It takes a word as a parameter. We are going to build a special stream on this word by using the chars method from the stream class. This chars method is a new method from the string class, new from Java 8, which returns precisely an IntStream, that is a stream of int primitive type. This IntStream is composed of all the letters of that word. Now we have here basically a stream of letter, we can map this stream of letter into a stream of score of letter by using this array. Let us do that, map, we take the letter and return the right _____cell of this array by computing letter - a, this is an old trick from the early days of the C language. This is an IntStream, so this map method is the map method from the IntStream and since I am dealing with an IntStream I have directly a sum method on it. So this function returns an int primitive type and this primitive type will be boxed into an integer automatically by the language itself. So I can also write this function in a different way, the specialized function, the ToIntFunction that takes a string as a parameter and that returns an int directly. So I do not put it in the template here and I am going to call this function intScore. It is the exact same implementation, but this time this int primitive type that is returned will not be boxed into an integer as was the case previously.

Live Coding: Getting the Word with the Best Score
So what can I do now with those two functions? Well I can, for instance, compute the score of hello intScore. apply and I pass the word hello as a parameter. Let us run this code. The score of hello is 8, which is effectively the case. H is worth 4 points, e 1 point, l another point, and o 1 point. So this is the correct result. So now how can I tell which of the words of Shakespeare is the best one? Well I have a set with all the words of Shakespeare, so let us open a stream on this set and let us just take the max of it by providing a comparator as a parameter. Now, how can I build this comparator? It's fairly easy. I am going to use the comparing static method from the comparator interface and I need to pass a function as a parameter that will take the current element of this stream, that is a word, and return a comparable object that will be used to compute the max, that will be used for the comparisons. And this is exactly this function that I need because I want to extract the max according to the score. So this is just the score function I need here. Now I need to be a bit careful because the max method returns an optional, we are going to see optionals in detail later in this course. So I need to call the get method here to have the results. The result is a given word of the stream, so it is a string of character and I can print it out, best word, like that. Let us run this code. The best word is honorificabilitudinitatibus, right. It is probably not a word accepted at Scrabble just because it is bigger than the Scrabble board itself. So I can add a filter step to remove all the words that are not in this set, which is the official Scrabble dictionary. Let us do that. Stream. filter and we need to keep the words that are contained in the scrabbleWords, contains words. If I write it like that I get a compile error because this variable word is already used here and I am not allowed to have this kind of variable name condition. So I am going to call this word bestWord to remove this compile error. So this time the bestWord is whizzing.

Live Coding: Computing Statistics on the Words of Shakespeare
But I can also do other things with the stream. Let us copy/paste this code here. We just keep the words that are allowed at Scrabble. By the way, this lambda expression can also be written like that, scrabbleWords::contains. It is another syntax called a method reference and this time instead of having a stream of integer, I am going to map it using this intScore, which will return me an intStream instead of a stream of integer. So let us call this mapToInt function passing this intScore as a parameter. And this time I have all kinds of new methods with among them this summaryStatistics method. This summaryStatistics returns a special object of type intSummaryStatistics and I can just print it out, Stats + summaryStatistics and if I run this code I will see that on one pass over the data I have the number of allowed words by Shakespeare, the sum of all the scores, the smallest score, the average, and the max score, which is the score of the whizzing word here. And if I want to go even faster, I can also simply call parallel here and this statistics will be computed in parallel. We will see later in this course how to implement this statistics computation and how to support parallel operations in them.

Live Coding Session and Module Wrap-up
Time to wrap up this live coding session. What did we see in the second live coding session? Well, we saw how to use the intStream specialized stream to compute the score of a word at Scrabble. It was fairly easy and we could play with this score function on the Shakespeare file and on the Scrabble official dictionary file. We also saw the use of the summary statistics pattern, which is only available on those specialized streams, IntStream, LongStream, and DoubleStream and which are extremely efficient. We will see more on the summaryStatistics pattern and especially we will see how to compute other statistics based on the same pattern. Now we are not quite done with the Shakespeare and Scrabble example, we will continue to work on it in this course. If you are yourself a Scrabble player, you might have notice a little problem with this whizzing word. We are going to work a little more on this subject. Now is the time to wrap up this module, so what did we see in this module? Well first we saw advanced stream patterns, flatMap, concatenation, merging, dividing streams into substreams, et cetera. Those patterns might prove extremely useful in our application. They are extremely efficient and are able to process large amount of data whether it is text data or not, by the way, in very short term, very quickly. We also saw what is the state of stream and this is very important to have that in mind when we design data processing pipeline. It is all about performance, all about improving the computation time of our data processing. And then we saw specialized streams, steams of numbers, once again introduced in the Stream API for performance reasons. That's it for the second module of this course. Thank you for watching and the next module is about advanced parallelization topic for the Stream API.

Parallel Data Processing Pipelines Using Java 8 Streams
Introduction and Agenda of the Module
Hello. My name is Jose. Welcome to the third module of this course, Parallel Data Processing Pipelines Using Java 8 Streams. The main topic of this course is about parallel computing using the Java 8 Stream API. Let us quickly browse through the agenda of this module. The first part is about parallel streams themselves, how we can go parallel using streams and what we can expect from parallel streams. We will go into detail about the difference between going parallel and going multithread because both notions have absolutely nothing to do, even if the first one, parallelization, is built on multithreading in the Java 8 Stream API. The second topic is stateful versus stateless operations. Now this is a very important point to understand, we are going to explain this into details because basically if you have stateful operations in a stream processing, you should not go parallel, it would prove completely useless. And the last point of this course is parallel reductions. The reduction step is the last step of the map filter reduce algorithm implemented in the Java 8 Stream API and we are going to see how we can build parallel reductions using this API (Loading).

Building Parallel Processing on Multithread
So let us first talk about parallel streams themselves. Why should we go parallel with the Java 8 Stream API? Well the first answer is quite simple and straightforward, to go faster in our computation for our data processing pipelines. Let us go into details on this topic. The first reason is to allow for faster computation. The second reason is also to leverage the multicore structure of our CPUs. Now our CPUs have many cores and they will have more and more cores in the future. If we want to go faster for a given algorithm, the only way to achieve that goal is to distribute our computation on several cores of our CPU. So this is what going parallel is about. It is different from going multithread. What does multithread mean? It means that each processing, it's data processing, is conducted in its own thread. Parallel processing means that a given processing, a given data pipeline, is computed among more than one thread. Think about the quick sorting of a set of integers, for instance, if I am in a multithread environment, each thread will be able to sort one given set of integers. If I am in a parallel world, it means that a pool of thread will able to sort one set of integer. This is a key point to understand at the very beginning of this part about parallelism and it has many impacts on how we program things. Why? Because in the case of the quick sort, for instance, the serial algorithm, the algorithm that will run in one thread, is not the same as the one that will run over more than one thread. So multithread is one process equals one thread, and I am allowed to have many processes at the same time. Think about a web server, for instance, or an application server, each thread will server one user, one request, or one SQL transaction. The problems I may come across in a multithreaded environment are well known; race conditions, that means many threads are trying to read and write the same variable or the same object; thread synchronization; and variable visibilities. In parallel one process equals many thread, just to go faster. And the problems I would come across are mainly algorithm problem, data distribution, among the CPU cores, and the balancing of the CPU loads (Loading).

Tools for Parallel Processing in the JDK
What are the available tools in my toolbox in the JDK toolbox? Well, the answer is really straightforward, up to Java 6 I do not have anything. If I want to do some parallel computing in Java 6 I have to program everything by hand. I don't have any framework whatsoever available in Java 6 to do that. In Java 7, the things are changing. I have a new framework introduced called the fork/join framework. Now this course is not about the fork/join framework, so we are not going to present that into details, let us say that the fork/join framework is a very technical framework, quite hard to master, even harder to debug when I have problems, issues in my computations. So it is merely a low level tool that I cannot really use efficiently in my applications. And I also have a 3rd party API called parallel arrays. It is Java 6 compatible. It has been introduced when Java 7 was released and it comes with its own embedded fork/join framework. So the fork/join framework from Java 7 is in fact made available for Java 6 application through this 3rd party API and this is why I am citing it here. By the way, this parallel arrays API has been written, has been developed, by the same people, the same team than the team that did the fork/join framework for Java 7. So it is really a solid framework that can be used in Java 6. And in Java 8 I have this wonderful tool, this magical tool called parallel streams. It has nothing to do with the fork/join framework, it is extremely simple to use, quite safe to use. We are going to see situations where it does not work that well, so I cannot really say it is completely safe. And it is build on the fork/join framework, so I can see that as a nice way to access the full power of the fork/join framework through a very simple API based on the streams.

Parallel Streams: First Patterns
Let us see how to create a parallel streams. We have in fact two patterns. The first pattern is to call the parallel stream method instead of the stream method and it will return a stream that is computed in parallel. See that the patterns of building the rest of the stream is exactly the same as the patterns we saw. So there is basically no difference here. And the second pattern is to call the parallel method on an existing stream. So basically a parallel stream and a nonparallel stream are exactly the same thing when it comes to writing code. This is really very nice. Now I must take note that since the stream is processed in parallel, if I run this code the order in which the people will be printed out is not guaranteed. Why? Because processing a stream in parallel means that the list of the people will be divided among the cores of my CPU and I cannot tell in advance which are the exact instances of person that are going to be processed first. So the forEach method will be called on the first ready instance of person from that stream, which is not guaranteed, of course. If I want to guarantee the order in which the people will be printed out I need to add a sorted code if my person class is comparable. This way the order of the elements is guaranteed. If my person class is not comparable, then I need to pass a comparator of person as a parameter to the sorted method.

Caveats in Parallel Processing: Synchronization and Visibility
Let us now see this notion of stateful and stateless operations. This is the main caveat when we want to go parallel and using stateful operation in a parallel stream can really kill our performances, how we are going to see it. What are the caveats using parallel streams? Because really written like that it looks like some magical stuff that we work all the time. And indeed it does most of the time, but there are cases when going parallel should absolutely be avoided and we are going to see that now. First, we must remember that parallel streams are built on top of the fork/join pattern on the fork/join framework and this framework is a multithreaded framework, so it means that my people will be processed in a multithreaded environment. Some things are to be avoided when computing things in a multithreaded environment and especially on the fork/join framework. I should not have any kind of synchronization and visibility issues. Remember we had a rule in the definition of the streams that was a stream should not modify its source. Basically if a stream processing modifies its source and you are going parallel, you will also go into big troubles, just do not do that.

Stateful and Stateless Operations: The Limit Method Example
But I have another thing that comes to mind is that we have a notion of stateful streams. That will not be computed efficiently in parallel. Basically I have two kinds of streams, stateless and stateful streams and if you are in a stateful streams environment, you should absolutely avoid going parallel. So what is a stateful and what is a stateless stream? Let us see an example of a stateless operation. This filter operation, person. getAge greater than 20, is said to be stateless. What does it mean? It means that to compute the result of this predicate I do not need any more information than the information that is held by this person instance. A stateless operation is an operation that works only on one element of a given stream and that do not need any kind of outside information. Let us see an example of a stateful operation. If I had a skip(2) or a limit(5) in my stream processing, then I am going into a stateful world. Why? Because I need to know that the element I am processing is the first or the second or the third, et cetera, element of that stream. So I need to have some kind of counter. This counter is shared among all the processings of all the elements and should be reliable, that is, every time I process one element I need to increment that counter. Now suppose that I am processing my elements in a parallel world, in a multithreaded world, it means that if my first element is processed in the thread t1 and my second element in the thread t2, then my counter has to be shared among those threads. It has to be either a synchronized variable or more probably an atomic counter. And this atomic counter will represent a choke point in my multithreaded computation. This is why stateful operations should be avoided in parallel and indeed if you are using skip or limit in the data processing stuff, do not go parallel, it is completely useless. You are not going to have better performances if you do that.

How to Tell a Stateless Operation from a Stateful One
How can we tell that an operation is stateful and an operation is stateless? Well first of all, it is written in the Javadoc. Just read the Javadoc of the methods of the stream interface that you are using and it is clearly stated in this documentation if the operation is stateful or stateless. And just as intermittent and terminal operations, with a little habit it is quite easy to tell. Basically the filtering we saw is stateless and it's very easy to understand and with a little habit we can see that the limit has indeed to be stateful. A stateful operation will just kill your performances in parallel, just do not do that.

Parallel Performance Analysis: Description of the Use Case
Now let us talk about performance. And let us see our first example of some code. What does this code do? It creates a list that can hold 10 million elements and it is going to create in a loop 10 million random long variables and add that to the list. So basically we are just generating 10 million random longs in a loop and we store those longs in a list. This code is not bound to be parallelized just because it uses the iterator pattern and I don't feel like going parallel with that code. Now let us see the second example of code. It calls the generate static method on the stream interface. Pass a supplier to this method that will be called again and again by this generate method and what does this supplier do? It just called the next long method, we just take the 10 million first element of that stream, collect the resulting list. So basically this second code is the same as the previous one, except that it is built on a stream. Since it is built on a stream it is going to be very easy to just add a parallel method called on that stream to go parallel and we have put explicitly a stateful operation on that stream, which is the limit method call. So we expect and we are going to see a performance hit on that code when it is executed in parallel. We can even write a third version of this code because the longs method of this ThreadLocalRandom current object can take itself a parameter, here 10 million, it means that we are going to select the 10 first million random values generated by this random. We map this stream into a stream of long from a long stream, so a stream of numbers to a stream of long, that is a stream of wrapper, just to be able to put it in a list of long. So once again, this code does basically the same as the first one, but with a stream. That stream is not built with the exact same pattern at the second version of this code. We will be able to call parallel on that stream to see if it works or not. Now we do not have any more limit call explicitly put on the stream, but we can image in that inside the longs call, which would have some kind of limit call that will have the same kind of effect or at least that is a stateful operation. This stateful operation might not be as explicit, as exposed, as in the second version of this code, but it has somehow to be there.

Parallel Performance Analysis: Understanding the Computing Times
So now let us see the performances, let us run that code, and measure the execution time. For the first code, the time is basically 270 milliseconds. For the second code it's a bit longer, a bit more than 300, and for the third code it's 250. Now we can see that the performances are roughly the same. The differences between the execution time is in fact not that much. For the first code we are not going to go parallel because it would be complicated to write parallel code on a for loop. But for the code 2 and code 3, which are basically stream code, it is very easy to just call the parallel method. Let us do that and let us see what it gives, 500 milliseconds for the limit and more than 300 for the longs. So going parallel did not give any better performance due to this stateful call in the stream processing. This is an excellent example of the impact of a stateful operation on stream data processing. It does not give any better performances if you go parallel. It's even worse than that because if I go parallel it means that I am using all the available cores of my CPU so I have killed the performances of my application, but I also killed the performances of all the other applications running on my computer. So this is really a very bad idea to go parallel in that case. In fact, the for loop might be the best idea and if you want to go to a stream, do not use limits, but use ThreadLocalRandom longs with the number of longs you want to have as a parameter.

A Sneaky Stateful Operation: The unordered() Method
Now let us take this example, which is quite tricky. The question is, is this operation, is this processing stateful or stateless? I build a stream on a list, I filter the elements of that stream using a predicate person. getAge greater than 20, and I just print everything out one by one. This is conducted in parallel, but it has nothing to do with the fact that this processing is stateful or stateless. Now the fact is, this people list is in fact an array list. Arrays. asList build an immutable list and since this is a list the stream built on that list is ordered. The ordered bit is set to 1 and this stream is an ordered stream. And since it is an ordered list it makes the operations stateful. This is a hidden statefulness and this is why I call this example the sneaky stateful operation. Because in fact, no operation on that stream are stateful. What is stateful is the stream itself. Do I have a way to fix that? Well in fact, yes I have a way to fix that, just call unordered on that. It will set the ordered bit to 0 and make this operation stateless. Calling unordered on that will relax the ordering constraint and make this operation stateless and much more performant in parallel because I do not have to carry the order of the list across all the operations on that stream.

Parallel Reduction: What Not to Do!
Let us talk now about parallel reductions. The reduction step is the last step of the map filter reduce algorithm and the Stream API in Java 8 is about implementing this map filter reduce algorithm. The last step, which is the reduction step, is with no doubt the more complex one. And it is also the more complex to go parallel with. Going parallel in the map and the filter step is just straightforward, there's basically nothing really to take care about apart from the notion of stateful and stateless operation. It is not the case for the reduction step. So what does it mean, use collectors instead of the reduce method? Well, let us see this code. What are we going to do here? We take a stream of instances of person and we call the reduce method with two parameters, basically a new ArrayList, that is an empty list, and a B function that takes an instance of this ArrayList and that will accumulate the ages of those people in that list. Why should not we go parallel with such a method call? Well we need to remember that parallelism in the Java 8 Stream API is built on top of multithreading. So basically we will have several threads executing on different calls of my CPU that will execute this B function on this instance of ArrayList and ArrayList is not a concurrent aware structure. It is not a thread safe implementation of the list interface, so race conditions will occur on that object. Different thread will try to add elements to this ArrayList without any kind of synchronization, so it will just not work. I will observe two kinds of corruptions, the first kind is the loss of ages in that list, suppose I have a hundred people, I might end up with only 90 ages in this ArrayList or even worse, I can also array out of bound exceptions if a thread is interrupted in the middle of the resizing of this ArrayList. We will see that in the live coding session, just do not do this kind of stuff, it will not work, it will crash your code. What should I do? I should go with the collector pattern, which is this one, call the collect method and pass the Collectors. toList called to this collect method. It will just work out of the box. It will do exactly what I need and if I want to go parallel, I can because the Collectors. toList will be aware of that and will handle parallelism and synchronization for me. This code is perfectly thread safe and will work perfectly. I am not going into more details on this pattern because we will see it at the end of this course. At the end of this course you will know everything you want to know about the collector pattern.

Tuning Parallelism: Setting the Size of the Common Fork Join Pool
Now just a last remark about parallelism, the first question that comes to mind is, is it possible to finely tune parallelism? How many threads parallelism will take by default? And how I can tune that? Do I have the hand on this kind of thing? As we said, parallel streams are built on top of the fork/join framework and by default the fork/join framework takes all the available CPUs. Now most of the time this is not what I want, so I need to change that. In fact, starting with Java 8, we have a special pool of thread launched when the JVM is started, which is called the CommonForkJoinPool of thread. This pool of threads is always there and is used by all the parallel stream operations. We can control this pool by setting this special system property, Java. util. concurrent. ForkJoinPool. common. parallelism. Here is it set to 2. It means that the CommonForkJoinPool will have two threads for all the parallel stream operations. Now this is really a security measure all the web server and application servers should take before launching. Imagine launching a Tomcat with web applications in it and suddenly one web application decides to launch a very heavy parallel stream operation. It will just kill the whole server and kill all the web applications handled by this instance of Tomcat. So really, this property should be set systematically probably to 1 with all Java 8 non-streaming operations.

Tuning Parallelism: Setting the Executor of a Computation
But we can go further. We can also decide to launch our computations in our own pool. Let us take a list of person. Let us build a ForkJoinPool by hand and set it to two threads, remember by default it will use all the core, all the CPUs available on the machine. We can submit a task to this ForkJoinPool. The submit method of the ForkJoinPool object takes a callable as a parameter and here this lambda expression we wrote is indeed an implementation of a callable. A callable is a functional interface, so it can be implemented using a lambda expression. And this callable just conduct the processing of our stream in parallel. It takes the ages of the people, filter the ages that are greater than 20, leaving out the people younger than 20 year old, and computes the average on it. It is wrapped in a callable, submitted to that ForkJoinPool and thus the parallel operations of that stream will take place in that ForkJoinPool instead of the common ForkJoinPool. Notice the get at the end of this computation, this is the get from the future object that is returned by this submit method. So yes, it is possible to finely tune parallelism. It is probably a security measure in the case of nonstreaming operations and nonstreaming application servers and for other streaming operations we can still decide which operation should have many threads and which operation should have not.

Live Coding: Understanding the Multithreaded Computation
Now is the time for a little live coding session. What are we going to see in this session? Well this session is about seeing parallel streams in action. We are going to see very simple things in parallel. I would like to show you especially how does the distribution of the computation works and how the different elements of a data processing pipeline with the Java 8 Stream API uses the CommonForkJoinPool to go parallel. This CommonForkJoinPool is a special pool of threads, handled by the JVM and all the parallel computations of the Stream API are conducted in this CommonForkJoinPool. Now let us see the effect of parallelization of the Stream API. For that let us build a simple stream. We are going to use the iterate pattern. Let us start with a simple plus and let us add pluses forever to this first element. Let us limit our stream to 6 elements and print out the result for each, System. out::println. Let us run this code and indeed our stream is composed of plus, then two plus, then three, until we have six pluses. Let us first call parallel on this very simple stream and run the code again. What we can see is that the different elements of this stream are not printed out in the order they are generated. Why so? Because this code will be executed in the CommonForkJoinPool, which is a pool of threads handled by the JVM directly and I cannot tell in advance in which order every element of my stream will be processed, will be printed out. In fact, each of those elements have been processed in their own thread and I do not know this thread in advance. To know this thread I can add a core here to the peek method. The peek method takes a consumer, so a consumer takes the current element of the stream and does something with it. So let us print out this element, processed in the thread, and let us call the name of the currentThread we are in. So Thread. currentThread. getName. Let us run this code. And indeed, what I can see is that the for element is processed in a main thread, but the other are processed in those threads, ForkJoinPool. commonPool-worker-2, worker-1, et cetera. Here I have until worker-4, there are probably more threads in the CommonForkJoinPool. How can I handle this CommonForkJoinPool? Well for that, we can just copy/paste this property. Here I am going to limit the size of this ForkJoinPool to two threads. Let us run this code again. And indeed, I can see that my elements are processed either in the main thread or in the thread 0 or 1.

Live Coding: Parallel Reduction, How Can It Go Wrong?
Okay, now what about putting the result of these computations in an ArrayList, for instance? Let us build here a list of strings, new ArrayList, and let us modify this code here. This time I am not going to print out the result, but to add each string in the list of strings, so strings. add of s. Let us comment out the parallel call and let us forget about this big call. And what I want to display here is just the size of the list, strings. size. Let us run this code. Of course, I have six elements in this list. Let us go really further to 1000 limit, I have 1000 limit in this list, which is exactly what I expect. But now let us go parallel with that code, what is going to happen? Well, I have an exception and this exception is ArrayIndexOutOfBoundExceptions. Let us run this code again. Here no more exception, but I have only 955 elements in my list. Now what does those two errors come from? Well as we saw, this code here is being called in different threads from my CommonForkJoinPool, so this list, here strings, is accessed concurrently from different threads. The problem is that it is an ArrayList and an ArrayList is absolutely not thread safe, it is absolutely not concurrent aware. So this is here a race condition. I have a race condition in my code, thus the result is completely random. I might have any number of elements in my resulting list or I can also have this exception due to the fact that one thread has probably been interrupted while resizing this ArrayList. If I transform this code with a concurrent aware list, for instance, CopyOnWriteArrayList, this time I will not observe any problem. Why? Because this list is a concurrent list completely thread safe. Now do not use this code in production because the performance are just terrible. This is just an example to show you that when you use a concurrent aware list here, the problems we had with the ArrayList are fixed.

Live Coding: Parallel Reduction, Hint at the Collector Pattern
In fact, this code is clearly not the right one to put all those elements in a list. The right code is not this one, the right code is to use the collect method, pass a Collector to it, that I can build with the Collectors. toList call. The return of this collect call here is indeed a list and I can check the size of this list. The result is indeed 1000 and the computation made is completely thread safe and very efficient too. So this is the right pattern to use if you want to collect the element of a stream in a list. We will see the collector pattern in full details at the end of this course.

Live Coding Session Summary
So what did we see in this live coding session? Well in fact we saw two main things, first how parallelism is implemented in the Stream API in the Java 8 Stream API, and the most important thing to remember is that it is built on top of multithreading, so all the problems I may come across in multithreaded environment will be seen in the parallel stream API. We saw the right pattern to collect elements in a list, in a thread safe way and that is in a parallel friendly way. This was the collector pattern. And we saw especially which pattern not to use. The forEach method is not here to conduct a reduction in a container as we saw in this ArrayList. So do not do that and bear with us until the end of this course because the right pattern to use is the collector pattern, we just saw a hint at that pattern, but we will see it in much details before the end of this course.

Module Wrap-up
Now is the time to wrap up this module. So the main idea of this module is to show you how parallelism can speed up computations in the Java 8 Stream API and it really works in some kind of magic way. Because basically to go parallel all you have to do is to call parallel on an existing stream and that's it. All the algorithms will be conducted in parallel. All your data processing pipeline will use parallelism. We also saw what can kill performances in these kind of patterns and the first idea was the stateful versus stateless operations. You should not have any kind of stateful operation in a parallel processing pipeline because if you do, then it will kill your performances instead of enhancing them, so just do not do that. And we also saw how to configure our applications to keep control over parallelism. There are situations where you do not want your applications to go parallel. Think of a web server, think of an application server. On an application server, you certainly do not want your user to deploy a web application with some kind of parallel data processing pipeline in it. So to control that, you just have a special property we gave to set to 1. And we also saw how to have a special computation being executed in your own pool of threads. This is also possible so you can use it to finely tune and finely distribute your computations among the different cores of your CPU. We also saw a first hint at patterns to conduct parallel reductions and this was the collector pattern. As I already told you, we will see all the details about collectors before the end of this course. And with this, that's it for the third module of this course. The next module will be about optionals and advanced patterns to build error lists, data processing, pipelines built on the Stream API.

Building Errorless Processing Pipelines with Optionals
Introduction and Agenda of the Module
Hello. My name is Jose. Welcome to the fourth module of this course, Building Errorless Processing Pipelines with Optionals. We are still studying the stream API from Java 8 and this module is all about optionals, which is a new concept in this API. Let us quickly browse through the agenda of this course. First we will present what optionals are and why have they been introduced on the stream API. And we will see the first basic patterns of optionals, how to read them, how to open them, to check if there is an object wrapped in that optional or not. We will also see how to build optionals from scratch as well as getting optionals as results of method calls. And then we will see advanced users of optoinals. Optionals can also be seen as a special type of stream, yes, special type of stream, a stream which would hold only one or zero elements and once we've seen that we can see more advanced patterns, much more elegant, and much more efficient to compute.

Optional: A First Explanation of the Concept
But first let us talk about the basic concepts of optionals and why they have been introduced, basically to tell us that some kind of operation might have no result. Let us see these first very simple example. We have a list of people. We built a stream on this list. We map that stream of person to a list of integer, which are the ages of those people and we reduce the stream by getting the max of this stream. The reduce method we use takes a first element, which is the identity element of the reduction operation. Now since we are dealing with positive integers, the age of the people are all positive integers, 0 is indeed the identity element of the max operation on this stream. So we will have a result, which will be the correct result if we run this code. But suppose that instead of computing a max over a set of positive integers, I am computing an average. This time I have a problem because the average method has no identity element and since it has no identity element, I do not know what should be the value of this method if computed on an empty stream. Suppose that stream is more complex than this one with a filter call, for instance, that could empty it. Then this average call could be computed over an empty stream. What should be the value of the average if my stream is empty? Well the answer provided by the JDK is the result is wrapped in an optional. Here it is an optional int because the average method is defined on stream of numbers, those specialized streams we saw in a previous module. Thus the use of the mapToInt method instead of the map method. I also have optional of t, that can hold any kind of object. So the answer is, when I am not sure that a method has a return value, then I wrap that value in an optional. The concept of optional is there to tell that value might not exist, a value might not be there, and this is exactly the case for average. Instead of returning a corrupted value, I prefer to return nothing wrapped in an object. So this optional object can be seen as a wrapper type, the same kind of wrapper type we already have in the JDK with the integer double long, et cetera. The difference being an optional can be empty. There might not be any value in an optional.

Patterns to Use an Optional as a Wrapper Type That Can Be Empty
Let us see a first pattern on how to use an optional. An optional can be seen as a wrapper and in fact it is a wrapper that might be empty. So I have two methods, isPresent and get. To check if I have a value inside that wrapper and to get that value, if it is present. And if I have no value, well I need to write some code to handle this issue. So basically this is the classical way of using an optional. We can do much better than that and this is what we are going to see. There is a variant of this pattern that makes it a little more readable. I also have an orElse method and I can pass a default value that will be used and returned in case this optional is empty. So it just wraps the two previous calls inside just one method call. It only works if the notion of default value is defined in my application. And I've got a third pattern, orElseGet, which is a little smarter because it does not build the default value if that value is not needed. What I pass as a parameter is a supplier and this supplier will build the default value on demand. We provide a way to build that default instance instead of returning the default instance directly. This is much smarter and this is a pattern that we can use nearly everywhere in our applications. In that case, if I do not need that instance, I won't build it so I'm basically saving the overhead of building that instance.

Patterns to Build an Optional from Scratch
But there is a second type of patterns. There is another way, much smarter and much more efficient of using optionals. But before seeing those patterns, we need to learn how to build an optional from scratch. So far what we saw is optionals return by special method calls. We haven't built any optionals ourselves. So let us first do that. The default constructor of the Optional class is private. So I cannot call new on the Optional class and this class is final so I cannot extend it neither. So we cannot build an optional using the new keyword, it is just not possible. So how we can build an optional? We have several static methods. The first one is empty and will return an empty optional, very useful for tests, but not only for tests and we are going to use it later. And there is an of method that will build an optional on the given object. Now be careful because this of method will throw a NullPointerException if I pass a null parameter to it. So I should not pass a null element to this method. Thus I cannot build optoinals on null objects. Would it make sense to build optionals on null object? I guess no. And the people who designed the JDK definitely think the same. I have another method, ofNullable, and this time I can pass null values to this method. And what happens if I pass a null value? It does not return me an optional built on the null, merely it builds an empty optional. So this method will return an empty optional if I pass a null value to it. So I have three patterns to build optionals and I cannot build optionals on null value, which will prove very handy.

More Optional Patterns: The Map, Filter and ifPresent Patterns
And I've got also another family of methods on the Optional class and those last methods should remind you of something. I have a map method, which takes a mapper as a parameter. What does this method do? So first it is defined on an Optional of T. So it takes the object inside that Optional and returns another object, a transform object of type U, and this map method will wrap that return object by the mapper inside another optional of U. So basically, I can pass from an Optional of T to an Optional of U using this map operation. What happens if the Optional of T is empty? Well the returned Optional of U will also be empty. I also have a filter method that takes a predicate of T. So this predicate will return true or false. If it returns false, then this filter method will return an empty optional. And the same happens if the Optional of T, on which I call this filter method, is empty either. And the last method is an ifPresent method that takes a consumer. Basically that consumer just consumes the content of this optional. If this optional is empty, then the consumer will have nothing to do and since it does not return anything, nothing will happen. So basically, I have here three methods, map, filter, and ifPresent, that clearly look like the three methods map, filter, and forEach from the stream interface. So it leads to the question, is an optional a stream? Cannot should I see an optional as a special kind of stream? And in fact, this is what we are going to do.

How We Can See an Optional as a Special Kind of Stream
So we have two types of patterns on the Optional class. The first type of pattern sees an optional object as a wrapper and I can query that wrapper, do you have an element? What is the element you're wrapping? And I can also see an optional as a special kind of stream, a stream that can hold only one element or zero elements and I have another family of methods for that, map, filter, and ifPresent, and also a flat map method that we are going to see now.

Introduction to Advanced Optional Patterns: The NewMath Example
Let us see now the most advanced used of optional and let us see how by leveraging the power of this new concept we can write full data processing pipelines on the stream API with no null values, no exceptions, and parallel computations. And we are going to see that through an example with this class that I called the NewMath class. It provides two methods, one to compute the square root of a double and another one to compute the inverse of a double. Now, we all know that the square root is not defined for negative numbers and that the inverse is not defined for new numbers. What is done usually is if I try to compute the square root of a negative number, I might get an exception or if I stick to the standards, I will get a NaN, a not a number double of float, which is a kind of a hack to handle non-defined operations. But we can do better with optionals and in fact, those two methods wrap the result in an optional. If I pass a negative number to my square root method, I will get an empty optional and the same goes for the inverse method. What can we do with that?

Building a First Data Processing Pipeline with the NewMath Class
Let us see how we can write a data processing pipeline using this NewMath class. Let us suppose first we have a list of doubles with all the numbers we need to process and let us suppose we want to store the result in a result list. We can begin by opening a stream on this list of double and for all the elements of that stream, use the NewMath to compute the square root, this will return an optional, and if I have a value in that optional, add that number to the result. If I want to compute the inverse of the square root, then I need to chain a call to the inv method of my NewMath class and I can do that, in fact, with a flatMap call. Now we haven't seen the flatMap method yet, but it works just like that, by chaining the call to the square root and to the inverse operation. Now, what is the problem with this way of processing things? In fact, there are two problems, first I am accumulating the result in an external list. This list should be final and I should be the only one to modify it. And since I am the only one able to modify it, I cannot go parallel. I can call parallel on that stream or use this way of processing data with a parallel stream, so I'm really missing here one of the most interesting parts of the stream API (Loading).

How Does the Optional.flatMap Pattern Work?
Before we go any further, let us focus a little on this flatMap method. Of course it looks like the flatMap method from the stream interface and indeed it does the same kind of thing. First, let us have a look at its signature. It is defined on an Optional of T, it takes a function that takes a T and returns an Optional of U, which is called a flatMapper. So it takes the content of the optional it is called on, this object is of type T. It maps this object into another object of type U and then it wraps this object into an optional that is returned by the flatMap method. Now the thing is, the original object of type T might not exist, the transformed object of type U might not exist neither, so this flatMap method is free to return an empty optional. It really looks like the flatMap method from the stream API and indeed it does roughly the same thing. Would it be possible to convert our Stream of Optional into a Stream of T so that we can leverage parallelism in the previous pattern we wrote? The answer is yes, but it is a little tricky and it is built on the use of this flatMap method. So what we want to do is to convert an Optional, which can hold an object, or that can be empty into a stream that would have the object inside this Optional if it is here or that would be empty if this Optional is empty. And once we had that stream we could use the flatMap method to flatten those stream of streams instead of having a Stream of Optional.

Leveraging the flatMap Pattern to Convert an Optional to a Stream
So let us build our example with this NewMath class that we just wrote. What we want to do is to build a function that will take an object, here it is a Double, and instead of returning an Optional of Double, which might be empty or might hold the result of the inverse of the square root, return a stream that will be empty if the computation cannot be made, that is if d is equal to zero or is negative or all the result in case it can't be computed. So let us do that. The first step, of course, is to call the NewMath. inv, which returns an Optional of Double. Now if we flatMap that result and pass this lambda expression, that returns an Optional of Double with the square root where we'll be able to compute the inverse of the square root. The result of those two calls is the following, it is an Optional of Double that holds the result if it exists and that is empty if it's not the case. What I want to do is to take this Optional and to return the stream in place of this Optional. So this is basically a mapping operation, takes an object and returns an object of another type. So let us call map and map is not the map of the stream interface, but the map of the Optional interface, so it returns the result wrapped in an Optional. So this mapping can basically return either an empty optional or an optional that holds a stream with the result of the square root of the inverse of d. Now what I can do is open this Optional. How can I open an Optional? By usisgn the orElseGet method call we can open an Optional. How can we do that? If there is a value in that Optional, this value will be returned and this is nice because this value is indeed the stream we want and if this optional is empty, if there is no value, then we want to return an empty stream that is not built. So what we need to do is to build that empty stream. And we can do it like that. The orElse method parameter is just a supplier that returns an empty stream. And this last call returns indeed a stream. This stream all the result we want, the inverse of the square root, if that result was indeed computed and this stream is empty, if for some reason the result was not computable. And this is exactly the function we were looking for. This function takes a double, returns a stream of double, that stream will be empty if the result cannot be computed and will hold the result if the result can be computed. If the inverse of the square root cannot be computed, then the stream is empty. If it can be computed, then this stream will just hold the value. So with this function we can convert an Optional into a Stream with the exact same properties as the Optional. And by the way, with method references we can also write this function like that, which is much cleaner and much easy to read. It might be a little tricky to write, but with a little habit it comes extremely handy.

Building a Parallel Data Processing Pipeline with NewMath
Okay so now we can write our data processing pattern again. We have a list of doubles and we want to compute the inverse of the square root of those doubles by leveraging the stream API. Now what I need to do is just doubles. stream to open a stream on this list of doubles, then flatMap the result with the function we just wrote, pretty simple, and since we have a stream I can now collect the result in a list using the pattern we have already seen. And this is very nice because now I am not using this ugly forEach call with an outside list, I am just building the list inside my stream data processing and of course I can safely compute this in parallel. So with this pattern I do not need to handle any kind of exception. I do not need to handle any kind of new values. Everything is conducted for me by the power of this Optional concept and as a bonus I get free parallelism.

Introduction to the Live Coding Session
Time for a little live coding example. Let us see this NewMath example in action and see how we can use Optionals to avoid having to end all NullPointerExceptions and not a number spatial doubles.

Live Coding: How Not to Process Doubles with NewMath
So let us build a pipeline of data. This data will simply be the stream of doubles and compute the inverse of the square root of those doubles. We want random doubles, so let us take ThreadLocalRandom. current and the doubles method from this random object, 10, 000, for instance, will be nice. Now this ThreadLocalRandom. current returns not a stream of doubles, but a double stream, that is a stream of numbers, for type reason we are going to convert that double stream into a stream of double using the boxed method on it. So I have a stream of double and I want to do compute for each double the inverse of the square root and get the result in a list. So I have prepared a list here and I am going to write my processing forEach of those elements. This forEach method takes a consumer. The consumer takes an object from the stream, here double, and does not return anything. So I'm going first to compute the NewMath. inv of this double. It returns me an Optional that will be empty if d is equal to 0. If I have a value, let us continue how a computation. So if I have the inverse of d here, what I want to compute is the square root of this inverse, just like that. And once again, this square root method will return an Optional, so if this Optional is there, if it is not empty, I want to take the result, which is the square root, and add it to my list, result. add of square root. So I have a piece of code here, which is not that nice with consumers inside consumers returned in a clumsy way and we are going to see that it does not work very well neither. Let us check the result and let us have a look at the number of element I have in my result list. Let us run this code. Alright, I injected 10, 000 random variables and at the end of the day I have 10, 000 variables in it (Loading).

Live Coding: Failing to Go Parallel with the Wrong Pattern
Now what I would like to do here is to compute this stream in parallel, since this is a stream and since I want performances. What is going to happen if I call parallel here? Now we have to keep in mind that parallelism in the steam API is built on top of the fork/join framework, that is, is built on top of multithreaded computation. So I will have more than one thread, in fact as many threads as the number of cores I have on my CPU. So on this machine it will be eight and those eight threads are going to call result. add concurrently in this list. Now this list is a basic ArrayList. It is absolutely not thread safe. It is absolutely not concurrent aware. So basically I am creating, in kind of a sneaky way, a race condition in the axis of this ArrayList. So what I expect to see if I run this code is random exception, lost elements in the process, and things like that. Let us run this code. Indeed I have an ArrayOutOfBoundException here with 163 as a value. Let us run it again. Once again the value is not the same. Let us run that. Now the value is 15. Run again. Again, another value. You can see basically that this code does not work at all. So I can use this pattern like that with calling parallel, each would work, but it will not be very performant and this way of writing code is certainly not the right way of using the stream API. So we need to forget about using this forEach method to loop over all the elements of a stream and do something with those elements, whether it is doing some computation as it is the case here or adding them to an outside list or a list defined outside of this loop, which is really an ugly pattern to use. Just don't use this pattern.

Live Coding: Building an Optional flatMapper for NewMath
Okay, so let us comment this code here and now let us write it in the right way that will allow parallelism. For that we need to leverage the flatMap method that exists both on the stream API and on the Optional class and for that we need to build a function that takes a double as a parameter and we'll return the stream of double. We are going to call this function flatMapper. So it takes a double d. I need to compute the inverse of the square root of this double, so NewMath. inverse of d, then instead of using an ifPresent and wrap the consumer in that, I am going to leverage the flatMap operation of that function to chain the call to the square root. So here I have the inverse and I want to compute the square root of this inverse. Once again, this returns me here an optional of double and what I want to do is to wrap the result of that optional into a stream to leverage the stream API instead of the optional API. So in the first place what I want to do is create an optional of stream in place of this optional of double. And the trick is to use a map method from optional and say if in this optional I have a double, which will be the square root of the inverse of d, I want a stream with this square root in it, that is a special stream with only one element in it. If this optional computed at that step is empty, well this map method will not be called, so I will be keeping an empty optional. Now what do I want to do with this optional? Of course I want to get rid of it, so I want to open it. If I have a stream in it, that is great, this is just the value I need. Now if this optional is empty, I want to convert it into an empty stream. So for that, I am going to leverage the orElseGet method. This method takes a supplier. A supplier is a special functional interface that does not take any argument and returns something and this something will be just an empty stream.

Live Coding: Analyzing the Optional flatMapper
And that's it. Here I take a double, I take the inverse of that double, returning an optional. If this optional is empty the flatMap will not do anything. The map will not do anything and instead of returning no value, which does not make any sense, this orElseGet method will be called and will return an empty stream. So I am converting an empty optional into an empty stream. Now if my double d goes through this computation, here I will return an optional with the inverse of the square root in it. This optional will be mapped using that method. What does it do? It takes this optional of d and makes an optional of stream of d and this orElseGet will just be called, but the supplier of course will not be used, so I will just execute a get method on that, opening the optional and returning the stream of d. So basically this flatMapper returns the result wrapped in a stream, which will be empty if there is no value to be computed and which will hold the value in case it is.

Live Coding: Parallel Stream with the Optional flatMapper
Let us take again our ThreadLocalRandom. current, doubles(10_000), now I will use the same trick that is boxed this stream to avoid issues with the type system and I will just flatMap the result using this flatMapper function from here. And now what can I do to create a result in a list, to collect all the elements and put them in a list? Well I am going to leverage the collect method and pass as collector to it, which is the Collectors. toList. We are going to go into details in the collectors in the next part of this course, bear with me, this is the right way of doing things. What does this collect method return? It just returns exactly what I need, that is the list of doubles. Let us call this list rightResult and let us check if I have the right number of elements in that list, let us run that. Indeed I have 10, 000 and it does not crash. Can I call parallel on this stream? The answer is yes. Why? Because this collector this time will use a list that will be built specially during the processing of my stream of data, so I will not have any issue running this code. So I want on the two sides, first I have a much more readable code than this ugly forEach and ifPresent nexted calls. Second, I can use the parallel stream here, completely, safely. Last point, this doubles call in fact create elements that are all positive and indeed I can see that I did not lose any element in the process, meaning that I did not see any negative doubles from here. So let us add some mapping here just to be sure that I have negative numbers that can be generated. So let us map this stream using this little trick. We are going to multiply our double by 20 and minus 10, so this time instead of dealing with a stream of doubles comprised between 0 and 1, I will have a stream of doubles between -10 and +10. So I expect to have less than 10, 000 doubles in my result list since some of them will be discarded by the computation of the square root. Let us run that and indeed I can see that I have roughly half of my doubles that have been discarded, which is perfectly normal since this is a random stream of doubles.

Live Coding Session Summary
What did we saw in this live coding session? Well in fact, we saw several things. First we saw how to efficiently use streams and optionals together. In fact, the right way of seeing an optional is to see it as a special kind of stream that can have 0 or 1 element and when we take this point of view, we can build very powerful, very clean, and very efficient patterns of code. We saw the importance of the flatMap pattern. The flatMap pattern is present both on the stream API and on the optional object and it also allows for the writing of very clean and very efficient code. We could leverage the full power of those two APIs by having them to work together. First on the very clean pattern, the code we wrote was extremely readable and extremely clean to write and we did not lose anything on the performance side. This pattern was very efficient too. We could leverage the full power of the stream API by using the parallel processing of our data.

Module Wrap-up
Now is the time to wrap up this module, so what did we see here? Well we presented the optional concept, this new concept introduced in the JDK 8. First we saw basic patterns, basically an optional is a wrapper type that can be empty, which is a difference from the classical wrapper types integer float, long, double, et cetera. And then we saw that the optional object can be seen as a special type of stream that can have 0 or 1 element. And this led to very powerful and very interesting patterns indeed. We then saw the advanced pattern built on optionals, which are the same kind of patterns as the patterns we have on the stream API, map, filter, ifPresent, which is a kind of forEach, and flatMap. And we could build an example where streams and optionals were playing together, playing very nicely to build an extremely powerful data processing pipelines. This pattern was very clean, very efficient, and could handle error in the very elegant and very natural way because basically in our pattern when one object cannot be process, it is just discarded naturally without breaking the natural flow of our data processing pipeline. And with this conclusion, I think that's it for this module. Thank you for watching. The two last modules of this course are about collectors and the next one is about the first pattern on how to use collectors for efficient reduction of our data processing pipelines built on the Java 8 stream API.

Collecting Data in Complex Containers Using Collectors
Introduction and Agenda of the Module
Hello, my name is Jose. Welcome to the fifth module of this course, Collecting Data in Complex Containers Using Collectors. Now this part is all about the last step of the map, filter, reduce implementation in the Java 8 stream API. We already saw patterns with the reduce method and here we are going to see an all new family of patterns based on the notion of collector. Let us quickly browse through the agenda of this course, first we are going to answer this question, what is a collector? Basically in a nutshell a collector is simply an object that will handle complex reductions for us and that will support parallelism. The second topic is about the collectors class. In fact, we do not have to build collectors directly. Collector from a technical point of view is an interface in the JDK, but we have this collectors factory class with a nice collector of static method that will provide the collectors we need for most of the cases we are going to come across in our application. And then we will see collectors in action. There is quite a lengthy live coding session in this course with a complex problem that is solved as an example and that will make heavy use of the collectors class.

What Is a Collector? A Look Back at the Reduction Step
First what is a collector? In a nutshell, it is a reduction in a container. Let us talk again about this last step of the data processing pipeline, which is called the reduction step. We saw reductions already in the previous parts of this course. They were basically aggregations in the SQL sense, sum, max, averages, et cetera. A collector can be seen and is indeed a special type of reduction, which is precisely not an aggregation. It is a terminal operation, so calling a collector through the collect method will trigger the computation of the stream, the processing of the data from the source the stream is connected to.

A First Anti-pattern to Reduce a Stream in a List
Let us see a first example, how to collect a stream of strings in a list. Let us take a list of string, which is peopleNames, let us build a result as an ArrayList. We can open a stream on the names of those people, then filter out the names that are empty, we are not interested in those names, and then add each string as a result using this forEach terminal operation. Now we already saw this example, or at least the same kind of example, when we were talking about optionals and we already saw what is just wrong with this way of doing things. Now what is wrong with this pattern? Well first of all, we are adding elements to an external list and we should not do that and we do not have to do that. Why? Because the list is final, that means we cannot modify its reference. We can still add elements to it, but we cannot access that list concurrently. Why? Because is an ArrayList and an ArrayList is not a concurrent aware implementation of the list interface. So we are missing a big opportunity for optimization, for increasing our performances, which is to build a parallel stream instead of a regular stream and this is just wrong, we do not have to do that. So this pattern should be avoided. Forget about adding elements from forEach method call.

Reduction in a List: The Right Pattern with a Collector
What we can do is instead of calling forEach, call the collect method on the stream interface, pass a special collector to it, and we are going to build that collector using a static method from the collectors class, Collectors. toList and it will magically return the list of string, which is the right result. And this way of doing things supports the parallel operation, so now we can call parallel to build a parallel stream. So forget about the forEach pattern and just remember the collect pattern, which is the right one to use in that case. In fact, this step is called mutable collection. We are collecting stream in a mutable way and why is it called like that? Just because the collection takes place in a mutable container, which is, in this example, our list.

The Collectors Class: We Have a Collector for That
Let us have a look at the collectors class and let us deep dive in what is a collector exactly? The JDK 8 provides a new factory classed called the collectors class. This class provides methods and structures to collect data and this collecting of data is about gathering the elements of our streams in a mutable container. What is this mutable container from a technical point of view? It can be a string of character and in this time, collecting data is in fact concatenating strings into a bigger string. Of course, it is implemented with a string buffer. It can be a basic collection, instance of the collection interface, that could be a list, a set, or whatever. And in that case, gathering data means adding elements to that collection. Or it can be a map, a HashMap, and in that case, gathering data means grouping data by a given criteria. We have collectors for that, it is something you are going to hear quite a lot in the next examples. So what is a collector? From a technical point of view, a collector is the interface that models each collector. The collectors is the factory class that is mostly used to build collectors. In fact, this collector interface we do not need to see it, we not need to study it, at least in a first time. Most collectors can be built through the factory. In the case we cannot built the collector we need through the factory, and it can happen, there are other more advanced patterns that we are going to see at the end of this part. But nevertheless, we don't have to directly implement the collector interface. There are static method from that interface that allows us to build the collector we need without having to implement that interface itself.

Computing a Max and an Average with a Collector
Let us see examples of collecting using collector. Basically we have a stream here of people. We call the collect method on that and the game is about finding the right collector to pass to that collect method to do what we want. So it we want to extract a max, we have a collector for that. The maxBy collector that takes a comparator as a parameter and that will do what we need, that is extracting here the oldest of the people of our stream. Of course we will fall back in the same kind of problem we had when using the max method on the stream of number, that is a max might be undefined if the stream is empty, so the result is in fact wrapped in an optional. This maxBy collector imposes the return type of the collect method and in that case it is an optional. We have the same kind of pattern to compute an average. We have a collector. averagingDouble that will compute this average and here we are going to compute the average of the ages of the people just in one line of code.

Building Strings with a Collector
What about collecting in a string of characters? Well, once again we have a collector for that. This is the Collectors. joining collector and see, that collector can take a string of character, which will be used as a separator in our string. Let us see that on an example. If my stream of people is composed of Barbara, Charles, Sharon, and Peter, then the result will be the names concatenated and separated using the string passed as a parameter to the joining collector (Loading).

Collecting Data in a Set
What about collecting in a set? Well once again, we have a collector for that, which is the Collectors. toSet. We have the Collectors. toList to collect our data in a list, the Collectors. toSet collects it in a set, which seems quite logical. Now these structures are concurrent aware, so of course I can go parallel with that processing. And if I need to collect in a custom collection, I also have a collector for that, Collectors. toCollection and I pass a constructor to that collection method in the form of a supplier. This lambda expression is an implementation of a supplier that will return a new TreeSet. And the return type of the collect method is imposed by the return type of that supplier.

Collecting in a Map: The partiionningBy and groupingBy Patterns
What about collecting in a map? In fact, we have several collectors for that. Let us see the first one, the most simple one, which is the Collectors. partitioningBy. This collector takes a predicate that can be either true or false, depending on each element of the stream, and returns a special map. The keys of map are Booleans, so I have only two instances of such keys, I have only two possible values for such a key, which is true or false, and the values are just list of people. If a given person matches the predicate, then it will be added to the list associated to the true key and if it's not the case, it will be associated with the false key. So this collector in fact does two things, first it takes all the element of the stream it checks if that element matches the criteria, matches the predicate passed as a parameter or not, and it regroups all those elements and associated that group to the true or the false value and collects them in a list. We can also do basic grouping. It's a bit more complicated. This time this map has an undefined key, well that is, defined by the function I pass as a parameter, this function takes a person and computes in our example, the age of that person and this will be the key associated to that value. Now of course many people will have the same age, so they are put in the list associated with that age. So the map is a map of integer, namely the age of the people, and the list of the people with that age, just in one line of code. This groupingBy looks like the SQL groupingBy and indeed it does basically the same kind of thing.

Adding a Downstream Collector to Process Map Values
But we can go even further. Suppose we are not happy to the fact that the values of this map are the list of the people. I can define a kind of post processing of those lists by passing a second collector to the collect method. Here the second collector is a Collector. counting. It will just count all the people in each list and return the result as a long. So the map becomes a map of integer, namely the ages of the people, and of value long, the number of the people with that age. Once again, all in one line of code. The second collector passed as a parameter is called the downstream collector, just because it processes the list of values associated with the keys constructed by the groupingBy collector. And we have other collectors usable as downstream collectors. For instance, I have a mapping collector that will map all the elements of those lists. Here the function takes a person and returns the name of that person, so instead of having a list of person, I will have a list of string as values for my map and those strings will be the names of the people.

Specifying Built Containers, the collectingAndThen Pattern
And I can go even further by passing a supplier to the mapping collectors and by telling instead of putting the names in a list, I want to put them in a custom collection built with the given supplier. Here my supplier builds a TreeSet, so the name of my people will be recorded in a TreeSet instead of a list. Now a TreeSet is a special kind of set. It is sorted in the alphabetical ascending order in the case of strings, so it means that my map will be a map of ages, integers here, and a list of the name of the people of that age alphabetically sorted. And it's not quite over. I can go even further by providing another supplier to the collect method, which will be used to build the return map. So in that case, this map will be a TreeMap, a TreeMap has sorted keys, here keys are integers, so the edges of my people will be sorted in the ascending orders and the associated names will also be sorted in the ascending orders. So this is just one line of code that enables me to build quite a complex structure with a nice use of several collectors. What about collecting in an immutable map? I cannot really pass a constructor to an immutable map just as a I did in the previous step. Why? Because this immutable map has to be built at the very end of the processing once all the key value pairs have been added to my map. I have a collector for that, special collector called the collectingAndThen collector and the AndThen means now that the map has been built, put everything in that case into an un-modifiable map. I give this pattern because this is the only way of returning an un-modifiable map using this kind of pattern.

Live Coding: Introducing the Shakespeare Plays Scrabble Use Case
Now it is time to do some live coding to see this awesome collector API in action. And this example will be about going further in the Shakespeare plays Scrabble example. Let us now see collectors in action and we are going to take the same example as the one we used when we showed patterns of the stream API. So we have our two files here, words of Shakespeare and the Scrabble dictionary. We have our array giving each score of each individual letter at the Scrabble game and we already built a function called score to compute the score of a given word at the Scrabble game. Let us run this code. We have roughly 80, 000 words in the Scrabble dictionary and 30, 000 words used by Shakespeare. Now what we want to do is to see the best words Shakespeare would have played if he had been a Scrabble player. Finding those best words can be done by building first an histogram, this histogram will regroup all the words by their scores, so basically it is a map of integers and list of string. Let us call it histogram of WordsByScore. How can I compute this histogram? Well I can just take the words of Shakespeare, build a stream on it, and leverage the collectors I have. There is one that is particularly well adapted to this problem, which is the Collectors. groupingBy. And I want to regroup the words by their scores. This is just one line of code to build this exact histogram. Let us run that. We are not going to watch this whole hashMap of course because there are 30, 000 elements in it, so it's too big to be watched like that, but what we can do is just check the number of key value pairs I have in it. Map. size, and indeed I have 36 key value pairs in it, that is 36 different scores with lists of words attached to them.

Live Coding: Building Histograms to Extract the Best Words
Now what can we do with this histogram? Let us see if we can have a look at the three best key value pairs from that histogram. Getting just a simple max is quite an easy task, but getting the three best key value pairs according to the keys is a little more tricky, so let us do that. Unfortunately on the map interface here I do not have any stream method. See, I cannot invoke a stream method on the map interface. So if I want to create streams on maps, I need first to take the entrySet of that map and then create a stream on that set. This entrySet is a set of Map. Entry of the type of the map, which is this one. So I can build a stream on this. Each element of that stream is an entry of this type. So from that stream, I can, for instance, sort that stream by passing a comparator as a parameter. Now what does this comparator need to compare? It needs to compare the elements of the stream, that is the entry of the type here, and I want to sort them in the descending order of their keys. So we can pass a comparator as a parameter, comparing and here I need to pass a function that takes an entry, the current element of the stream I am working on, and I need to extract the key from that entry, which is the element I want to compare to compare those entries. So entry. getKey, see like that. Now if I write it like that I will compare according to the key in the ascending order, the smallest first and the biggest last. I want the opposite, so all I need to do is add the minus sign here to have the keys in their descending order. Great, now I have a sorted stream and what I want to do is just take the three best elements, the three first elements, so limit. 3 and then, for instance, print out the result. So this is an entry, and I want to print the result out, System. out. println(entry. getKey) + a little dash sign to separate the key on the value, entry. getValue. So this little piece of code sorts the set of the key values pairs in the descending order of their key using this comparator. Here I take the three first elements, that is the three best elements from the key value pairs, and print out the results. Let us print that. And we can see that we have indeed three scores, 40 points for these words, 36 points for these, and 34 for those. Now we can see that the people who extracted the text from Shakespeare did it in probably a very brute way because they left the numbers of the chapters returned in Roman numerals. So we need to sort that out along with those words which are not allowed at the Scrabble game. So what we can do is just in the histogram here, add this filter step. That will remove the words that are not contained in the Scrabble words. So contains(word), and once again, I can write this as a method reference, just like that. Let us run this code once again and this time I have other words allowed at the Scrabble game, whizzing, buzzards, and a bunch of others worth 28 points.

Live Coding: Writing Whizzing with a Blank Letter
Now the question we may ask ourselves is, is the word whizzing really possible at Scrabble? Let us check that. We have a second array here that gives for each letter the number of available letters in the game. So we have 12 e, 1 j, 2 m, and in fact we have only 1 z. So the word whizzing can be returned in the game, but using a blank letter for the second z of the word. And the problem is that this blank letter does not score any points. So I need to do two things here. First, filter all the words that needs more than 2 blanks to be returned in the Scrabble, to be played in the Scrabble game. And second, to modify the scoring function, taking into account that if I have two z's, for instance, in a word, then the second z is bound to be a blank that will score 0 points. Remember the way we compute the score, we map each letter blindly to its score and sum of the results. So for the two z's, it will double the score 10, which is, of course, not correct. Now this problem might seem a little complex and indeed it is not particularly simple to solve, but in fact it turns out that it is still a map reduce problem, a histogram problem, and we can still leverage the stream API and the collectors to solve it.

Live Coding: Computing the Histogram of the Letters of a Word
So now let us do that. And first let us think twice of how to solve that problem. The question is, how can I count the number of blanks needed to write, for instance, whizzing? Well, I know that I have 1 z, 2 w's, 2 h, so what I first need to know is how many of each letter I need to write the word whizzing. I need 1 w, 1 h, 2 i's, one here and the second one here, 2 z's, 1 n, and 1 j. So basically what I'm describing here is a histogram, the histogram of the letters used to write the word whizzing. So I first need a function that will take a word as a parameter and that will return the histogram of that word as a result. A histogram is a map, so the result of that function will be a map. Letters in our problem are modeled by integers, so let's say that this map will have integers as their keys, and the numbers are modeled by longs. So this map will be a map of integer and long. And this is the histoWord that computes the histogram of the letters of that word. So it takes a word as a parameter. I will check that word letter by letter, so open the chars stream on it. This stream is of type int stream and since I want to put the result in a map using a collector, I will have to box it to make it a stream of integers. Now that I have the stream of integers I just need to collect that stream with a grouping by collectors, once again. This collector will take a letter and just return the letter to extract the key because the key is basically the current element of the stream and what I want as a value is the number of stream, that is Collectors. counting. And that's it. This simple one line function will compute the histogram of the letters of each word.

Live Coding: Computing the Number of Blanks Need for Whizzing
Now that I have this histogram I know that in whizzing I need 2 z's. To compute the number of blanks needed to write whizzing, I then need to compare for all the letters of that word, the number of needed letters to write that word, and compare this number to the number of available letters obtained by reading this array here. So I can create a second function that will take a string which is a word and return long, the number of blanks I need to write that given word. So I take a word as an entry, then I will compute the histogram of the letters of this word, histo apply of word. This is a map, let us write it here. This is a map of integer discussion long and I can even write it like that, it's a map of letter and number of letters. This is not a Java type, but this will help me in understanding the data I am handling. What can I do when I have a map and when I want to stream? Well, we already did that, we just call the entrySet of this map and build the stream on it. So this stream is a stream of Map. Entry of integer and long. The integers are the letter and the long, the number of letters needed to write that word. Now what I want to do is compute a number of blank. So for each of these element, I will take the letter, do some kind of transformation, and return the number of blanks needed for that very letter. So this is basically a mapping. I am transforming my stream. And this is a mapToInt because at the end of the day what I want to do is to sum up all the elements of that stream. So using mapToInt will convert this stream of entry into an int stream or merely a long stream and get the long as the result. Now the function I'm writing takes an entry as a parameter, which is the current element of the stream I am working on and what do I need to compare from that entry? The number of letter needed to write that given word, which is basically entry. getValue, and I'm going to write it like that, the number of letters available in the game computed by reading this array at the right cell. So this time it is here, entry. getKey, this is the letter, - a, always the same trick from the old days of C and C++. Now what can I do with those two values? This will be 2 for the z in whizzing and this will be 1 for the z in the Scrabble game. What I need to do is just compute the minus between both. If the value is positive, this is the number of blanks I need. If the value is negative, well it means that I do not need any blanks. So what I need to do is take here the max between this value and 0, just like that. This method will compute the number of blanks needed to write a word in the Scrabble game. We can just check for whizzing, how it works, # of blanks for whizzing, nBlanks, apply to whizzing. And indeed I need one blank to write whizzing.

Live Coding: Computing the Score of Words with Blank Letters
Now I need a second function to compute the new score of whizzing, for instance, taking into account that I have one blank in it. So this function will take a word, return a score, so an integer, and I am going to call it score2. It takes a word as a parameter. We'll compute the histogram of that word. We are going to process that histogram as a stream, so once again entrySet. stream. What do we have here? Basically exactly the same type as the previous one. Now how am I going to compute that score? I will change my algorithm, of course. What I can do is take the number of letters, I have 1 w, 1 h, 2 i's, 2 z's but the second one is a blank, so that is in fact 1 z, and multiply this number of letters by the score of each individual letter. So I am going to do the same kind of thing as previous one, this time I want to return an integer. So I will map my stream to int, make the sum of the result, and in this mapToInt I need to take the score of each individual letter, which is obtained by reading this array here. So this mapToInt takes a function that takes an entry, I am going to compute the score of each individual letter entry. getKey - a, multiply by the number of letters needed to write that word and to have this number of letters I need to compare once again entry. getValue with the reading of this array in the right cell, which is the number of available letters and this time I will not do any kind of subtraction, I just need to compare those two values and take the smallest of the two, that is integer. min of entry. getValue. This is a long, so I need to take the intValue and the number of available letters in the game, and that's it. This is my score2 function. And I can check the score2 for whizzing, score for whizzing. This is the first way of computing the score and I can also compute score2 of whizzing to see the difference and indeed I can see that in the first version the z has been counted twice and in the second version it has been counted only once.

Live Coding: Computing the Best Words with Blanks
Okay, so let us take our histogram by score here, modify it of course a little, this will be ByScore2. I take the stream of words of Shakespeare, filter out all the words that are not allowed at Scrabble game, filter again all the words that need more than two blanks to be returned and blanks apply word has to be lesser or equal than 2, grouping by score2 instead of score1, and then I can chain the rest of the computation, entrySet, stream, sorted, this will be exactly the same, so I can just copy/paste that code here. Let us comment out that code and just run this piece of code. And indeed this time I can see that whizzing and buzzards have been removed just because the second z did not score anything and now the best of the words is squeezes with an s and the second one is quickly. So we can see that this collector API along with the stream API makes an extremely powerful tool in the Java space. With those two new tools we are really very well equipped to build pipeline of data processing in Java 8.

Live Coding Session Summary
So what did we see in this live coding session? Well we saw in fact many things, first we saw how to set up a data processing stream. We took quite a large set of data, all the words by Shakespeare and the Scrabble official dictionary and then we could build streams on those files to process the words that are contained in them. We collected the result in a HashMap using the groupingBy pattern, which is really the most popular pattern of the collectors. We could build histograms and, of course, once you have histograms, you need to further process the data to extract meaningful information. So to do that, we built streams on those HashMap using the entrySet trick. We do not have a stream method directly on HashMap, but we can extract an entrySet, which is a set on the HashMap from the JDK and on this entrySet we can build streams. And then we could extract the meaningful information using postprocessing. First we sorted the streams by passing a custom comparator, it was very simple, and then from that we could get the first elements of that stream just to extract the best words we were interested in.

Module Wrap-up
Now is the time to wrap up this module. So what did we see in this module? Well, basically it was all about collectors and collectors in action. Collectors are used to reduce streams in containers and we saw three containers, the string, the collections, and the maps. We could concatenate strings along with separators and this could prove very useful if we have many strings in a stream. We could build collection with the three pattern toList and toSet and if you want to build your own homemade collection, well you can pass the constructor of your collection to the toCollection collection. And then we could build HashMaps with basically two patterns, partitioningBy, which builds HashMaps with Booleans as keys, and groupingBy, which looks like the groupingBy we have in the SQL language. And we saw that it was possible to completely customize the values of those HashMaps from the list, which is the default behavior, using downstream collectors, and those downstream collectors are a very powerful feature that will enable the building of very complex maps. And then we saw collectors in action on the Shakespeare example. Now of course this example is a toy example, but we could build very complex structures in it based on the use of histograms. Now histograms have a very classical structure to build when you have data to analyze, so it was very useful to see that. We could open streams on HashMap using this entrySet trick to postprocess the data of our histograms and all of these, as a bonus, can of course be conducted in parallel. Remember, this is the Java 8 stream API, it supposes parallelism and all those collectors can be called in a parallel environment. Well, that's it for this fifth module. Thank you for watching. The next and last module of this course is once again about collectors and in this last module we are going to build custom collectors in case those available in the JDK, in the collectors class, do not exactly fit your needs.

Building Custom Collectors for Advanced Data Processing
Introduction and Agenda of the Module
Hello. My name is Jose. Welcome to the sixth and last module of this course, Building Custom Collectors for Advanced Data Processing. Now the previous module was about collectors, but it was about standard collectors, the collectors available in the JDK through the collectors factory class. In this module we are going to see how to build our own collectors in case the one we have in the collectors factory class are not enough. Let us quickly browse through the agenda of this course. First we are going to see how to build custom collectors with the three fundamental notions, the supplier to build the resulting mutable container, the accumulator, and the combiner. It might remind you of other things we have seen in previous modules and indeed, we already saw these kind of things in other context. And then we will see and extended live coding session to build a complex custom collector to process a complex data stream on a real and complex example. This is the movies and actors example that we will present at the end of this module.

What if We Need Collectors that Are Not in the Collectors Class?
So we saw how to use ready to use collectors to build lists, to build maps, to build strings of characters. We saw how to use them in very powerful pattern and since the collectors class has so many static methods, most of the time you will not need to go beyond the use of this collectors class. Now suppose that we need to use a collector that is not present in the collectors class and that we need to build ourselves. Of course, there is this collector interface that we could implement, but in fact we do not need to do that and we do not want to do that. We can still build a collector using another way. So far we saw only ready to use collectors. Of course there are cases that do not fit in this catalog. There are cases where we really need a way to build our own collector and we are going to see in a further live coding session an example of that.

Collectors Made of a Supplier, an Accumulator, and a Combiner
A collector is in fact made of three elements. The first element is used to build the resulting container, we saw a string of character and there is an intermediate container, which is of course the string builder or the string buffer, but it can also be an ArrayList or a HashMap or whatever instance of the collection framework. The second element adds an object from the stream to the resulting container. For instance, it can be adding an object from a stream to the ArrayList and this just an add operation or adding it to a HashMap, this is a bit more complicated, especially in the case of the groupingBy collector. And in fact, those two elements are enough to build a simple collector. If I want to go parallel in my data processing, I need a third element, only used in the case of parallelism, which are used to merge together two partially filled containers. Parallel is about distributing my computation among more than one core of my processor, of my CPU, so each of those cores will have to handle a certain part of my computation and when each of these cores is done with this partial computation, I have to merge the results together and this is what this third element is for.

Example of a Supplier and Accumulator for an ArrayList
Let us take an example and let us build an empty container in the case of an ArrayList. How does it work? Well I take nothing since the building of an ArrayList doesn't take any argument, and I return an empty ArrayList. Now this is just a supplier, a supplier does not take any argument at all and returns a fully built ArrayList just by calling its constructor. Now we need to define how to add an object to the container, that is in our case an object to an ArrayList. We have an ArrayList with already four elements in it. It's an ArrayList of person and we want to add a new instance of person in that ArrayList. This is called the accumulating step. It requires an accumulator. How can I model? How can I write such an accumulator? It's pretty simple. It is just a BiConsumer that takes a person, a current element of my stream, and an already partially filled container, which is a list of person. I merge those two elements together just by calling list. add of p. Very simple.

Example of a Combiner for an ArrayList
And now the third step is the more complex one. It is about merging partially filled containers. In the example of the ArrayList I have a partially filled ArrayList built on my first core. Let us call it CPU 1. I have another partially filled ArrayList on CPU 2 and maybe on CPU 3, CPU 4, et cetera, I have other partially filled ArrayLists. Now what I want to do is to define an operation that will merge together those two partially filled ArrayLists. It is really simple in fact, I need to add all the elements of my first and my second list into, let us say the first list in that way. This last step is called the combining step. It requires an element that is called a combiner. How can I write a combiner? Well, very easy. A combiner takes two lists, list1 and list2, and has to return a third list that is the result of the merging of those two lists. Now how can I merge two lists together? Well I have a method for that, it is the well known addAll method, and I can return the first list since this is a list in which I have made the adding.

Putting it All Together: The Collector.of Pattern
So a collector can be built out of those three elements put together with a special static method from the collector interface, which is the of method. The of method takes three arguments. The first argument is just the supply, new ArrayList. The second argument is the combiner, list. add(person). And the third argument is just the combiner, list1 and list2 that returns list1 by the operation of adding all the element of list2 into list1. Now we are not quite done yet. We need a third argument to the building of the collector. This argument might look a little weird. It is a characteristics argument. Remember the characteristics of the spliterator? This is the characteristics of the collector. It is a little more simple than the ones of the spliterator and it is the same idea behind that. In fact a collector has three characteristics, IDENTITY_FINISH, CONCURRENT, and UNORDERED. Concurrent means that this collector can be called in parallel. Unordered means that the order doesn't matter in this computation. Here we just pass identity_finish, which can be seen as the default value of all collectors.

Live Coding: Presenting the Actors and Movies Use Case
Great. Let us have a look at that in a further live coding session. We will call it the Actors and Movies example and you will see that this live coding session really leverages the full power of the collector API. Let us now see collectors in action. In this live coding session we are going to use collectors and we are going to build our own collector on the complex example using the collectors. of method. We are going to work on the data set called moves-npaa. txt, which is a data set of movies and actors extracted from the well known imdb database. There are about 14, 000 movies in this data set that we're going to analyze using streams and collectors. We need two classical Java beans in this example, an Actor bean with two properties, lastName and firstName, very classical bean with its hash code and equals method and a two string method to display it. And we also have a Movie bean, same kind of bean, with a title, a releaseYear, a set of actors, and a two string method. Now I already wrote some code to analyze this file using the Files. lines factory method passing a path as a parameter and we are going to analyze this file line by line with this forEach method that takes a consumer and that will basically split this line and analyze its content to build a movie and the set of actors that played in that movie. Basically we have about 14, 000 valid movies. We didn't take all the lines of the files just to simply the analysis of the movies. And the first question we would like to answer is how many actors have been playing in those movies?

Live Coding: Counting the Number of Actors
So let us write that code. Number of actors in our base. Remember that we do not have a database of actor in itself, the actors are just referenced in each movie. So what we can do is take the movies, open a stream on that, and just map the content to get the actors. Here we have basically a stream of set of actors. How can I count the number of actors in this stream of set? Well I can use something we saw in a slide and in previous parts. I can change the stream of set into a stream of stream just by calling the stream method here. And what can I do with a stream of stream? I can flatten it using a flatMap call instead of a map call and this time I have indeed a stream of actors. Now what can I do with this stream of actors? I have basically two patterns to count the number of actors in that stream, a right one and a wrong one. Let us see first the wrong one. The wrong one would be to collect the results in a set by calling collect(Collectors. toSet) and to compute the size of the set. The result is the correct one. I will have indeed the number of actors in all my database, but I am basically building a set that I'm not using. So I should not use this kind of pattern because it is too costly, instead if all I need is counting the element of a stream, then I need to call a distinct method and count the resulting elements. The results will be the same, but this time computed in a much faster way. So this is the numberOfActors, I can write the results, # of actors, nubmerOfActors. Let us run this. I have roughly 170, 000 actors in my database.

Live Coding: Finding the Actor That Played in the Most Movies
Now we need to be careful at this step. Why? Because obviously in this stream of actors I might have doubles. One actor may appear more than one time in that stream. Why? Just because a given actor might have played in more than one movie in this database and if it is the case the way we built this stream will make this actor appear in that stream more than once. In fact, we can leverage that to compute the number of actors that played in the greatest number of movies. Let us do something a little more complicated and compute the number of actors that played in the greatest number of movies. And let us take this code again, movies. stream flatMap here. What can I see that if an actor played in two different movies he will appear twice as an object in the stream. So I can just leverage this fact. This time I am not going to call distinct, I will keep the doubles in that stream and just build a histogram of that stream using the classical collect and Collectors. groupingBy. I want to regroup all the elements of that stream by actor, so this is a stream of actor. I want to keep the actors as they are and I just want to count them, so passing a downstream collector, which is Collectors. counting. What does this result give me? Well, it basically builds a map, the keys of that map are the actors, and the value, the number of times that actor had played in movies from my database. So what I need to do now is just to extract the max of this map according to the value. There is a trick to do that. First we need to take the entrySet of this map to build a stream on it. This time we have a stream of map. Entry, this is the basic type of the entrySet and the type of the entrySet is simply the type of the map here. This comment here is just here to help me understanding the exact type of the data I am using. Okay so this is a stream and I want to get the max of that stream. The max takes a comparator as a parameter, so I can just compare using this lambda expression, entry. getValue. The max returns an optional so, of course, I need to call the get on that optional to open it. Let us comment out this code. Insert the result in a variable, this is mostViewedActor. But in fact, I can do even better because this comparator has been coded in the map. entry interface itself. If I check the map. entry interface, I can see that I have four existing comparators and one is exactly the comparator I need comparing by value. So this is even more readable code than the previous one. And here I can also improve my code a little just by calling function. identity, which does too exactly the same. Okay, so let us print out the result now, the most viewed actor is, the result is Mr. Frank Walker that appears in 90 movies from this data set.

Live Coding: Adding the Year Constraint, Introduction
Let us go one step further and try to tackle the real complex problem. Now I want to have the actor that played in the greatest number of movies during a year according to my data set. Now this problem is really tricky and the difference between this one and the previous we have just solved is that I do not have a ready to use collector to solve it. So to solve this problem I am going to create a map. The keys of this map are the release years of my movies, this I can do it quite easily, but the values are another map and this time a map of actors and # of movies during that year. This is the map I want to build and the hard part is to build this map. Why? Because I do not have a ready to use collector to build that map since the actors are in fact inside the actors field of the movie object. So I need to kind of inverse the way I am looking at this data set and it is quite tricky to do. Let us build a data processing pipeline to solve this problem. Of course, my starting point is my movie set. I can open a stream on this movie set and the first step is to collect that stream using this Collectors. groupingBy. I want to regroup my movies by release years, so this is fairly easy to do, movie -> movie. releaseYear and then I need to pass a downstream collector to build this map.

Live Coding: Setting the Custom Downstream Collector
The downstream collector I am going to pass as a second parameter is going to be built using this Collector. of method. Let us see that. Now remember this Collector. of method creates a collector using the three elements I need to provide to build a collector. The first element is a supplier to build the resulting immutable container that will hold the result. The second one is the accumulator, it takes a partially filled mutable container and an element from the stream and add that element to the container, and the combiner, that will take two partially filled containers and merge them into one.

Live Coding: Writing the Supplier for the Custom Collector
What is the supplier? Well the mutable container I am building is just this element, so it is basically a HashMap. A supplier takes no argument and returns the build mutable container, so it is just a constructor of this HashMap. Now I need to provide the exact type of this HashMap. The keys are actors, so the key is quite easy to set up. And then the value is the number of movies this actor played in during that year. Here I could take a long and it will work, but in fact, I need a mutable long, a long that I will be able to increment when I meet this actor in a given movie. So instead of taking a long, just for convenience, I am going to take an AtomicLong. An AtomicLong is normally used in concurrent programming, here I am not writing any code of concurrent code, but on that object I have methods like increment and get that I am going to use to increment that block.

Live Coding: Writing the Accumulator
Now it's time for the accumulator. This accumulator takes a partially filled container of this type, that is a map. So let us write it like that. The type of this map will be, of course, this one, and the second element is a given element of the stream I am processing and this is a stream of movie, so this is my second element. Now what am I going to do with my map and movie? Well I am going to write some processing. Basically I need to loop over all the actors of that movie and add the given actors to the map I am building. So this is a stream problem, movie. actors. forEach, and for each of these actors I want to take this actor and add it to the map, which is this one, so actor -> map. And here I am going to leverage the new method from the map interface. If I use the classical method I will end up writing this kind of code, actor and new AtomicLong of 1 if this actor is not already in the map, and if this actor is already in the map then I need to get the existing AtomicLong and increment it. Instead of using that put method, I can use the computeIfAbsent method, that will do exactly what I need. I want to put this actor, if this actor is not present, then I want to create a new AtomicLong, if this is actor is not present, then this function will not be called and on either case, I want to increment the resulting AtomicLong, and precisely this computeIfAbsent method returns the current value, either existing, either just built using this function. So here I can call incrementAndGet to increment the AtomicLong I just add.

Live Coding: Writing the Combiner by Merging Maps
So this is for the accumulator. Now I need to create the combiner, just as we did. The combiner is supposed to combine two existing maps of this type that have been partially filled on each core of my CPU, remember the combiner is used in case of parallel streams. So I'm going to call them map1 and map2. And do something that will merge the content of map1 and map2 into one HashMap, of course of the same type, and return this HashMap. So basically what I can do is just return map1 and merge the content of this map2 inside map1. So let us do that. We are going to loop through all the key value pairs of map2, so take the entrySet and then for each of those key value pairs do something. I am going to call this entry entry2 because this is an entry from map2 and what I want to do is merge this entry inside map1 using the merge method from the map interface. This merge method takes two parameters, first the key I want to add into map1 and this key is of course entry2. getKey and add the existing value, entry2. getValue. Now if I already have this key inside map1, what I need to do is merge the value from entry2 and the existing value in map1. So what I need to do is provide a lambda expression that takes two AtomicLongs, one from map1 and the other one from map2. Let us call them al1 and al2. And they will return al1. addAndGet, al2. get and since addAndGet returns a long, what I need to do is return al1, so I need to wrap that into a block of code, return al1. So here is my combiner. Take two maps, partially filled, loop over the elements of the second map, add them one by one using the merge method to map1 and then return map1.

Live Coding: Taking a Global Look at the Custom Collector
Now a collector takes a fourth argument, which might look a little weird and that's what we're just going to copy/paste here. I am not going into much details into this Collector. Characteristics. IDENTITY_FINISH, I just need to add that to that method. Alright, so what do we have here? We have this collect call with this huge Collectors. groupingBy that ends up here, let us do some indentation here. This is the parenthesis of the collect method. This groupingBy takes basically a key extractor that is just the releaseYear and then a custom collector as the downstream collector. This downstream collector is in fact composed of three main elements, the supplier that is going to build the resulting mutable container, the accumulator that takes an element from the stream, remember this is the downstream collector so this stream is built on the lists of this groupingBy collector, and a map, which is of course this type of map, and adds that movie to this map. And this is where I am going to analyze the actors of that movie to add them one by one to this map. And then the combiner, which is used in case I am going parallel, basically it's the merging of two maps, two partially filled maps, one on each core of my CPU. And then there is this fourth element, which is a little cryptic and we are not going into details about this fourth element. So now what do I have? I can just ask my id to help me a little on this one. I have exactly what I need, that is a map of integers which are the release years and a HashMap of actors and AtomicLong which are the number of movies this actor played in.

Live Coding: Extracting the Max of the Submap
So now what we need to do is to extract the max of this map, remember this is a map of map according to the AtomicLong here. And I will have the releaseYear, the actor, and the number of movies that actor played in. Let us do that. Let us continue our processing. We are going to copy/paste the type of this map here just to have it at hand. So what I need to do now is to first transform this map of key and HashMap, which is not very handy, into a map of keys and entries. Basically I would like to have this type and the end of my further processing, instead of having a HashMap here, I would like to have a map. entry here with the correct key value pair there, that is the best actor according to this value, of course, during that year. To do that, of course I have a map, so I need to take the entrySet and the stream once again and I am going to use another collector, which is the Collectors. toMap. Now this collector takes two elements, the first one is used to extract the key from the stream here. The first element extracts the key from this big entry here, map. entry here, the key is not changing, it is still the releaseYear, so entry. getKey here. And the value will be computed out of this value there by extracting the max of this HashMap, considering the AtomicLong, which is here. So the value is an entry. getValue. This is the map that I have here and I want to extract the max, so I am going to open an entrySet on the stream on this entrySet once again, so entrySet here. stream, and take the max given a special comparator that we are going to build, this max will return an optional and we are going to open this optional here. So what do we put in this max? Well of course a comparator. Now we are going to compare the values of this map according to the AtomicLong here, so we can use the map. entry comparing by value. Now the problem is that AtomicLongs are not comparable, so I need to provide a comparator to compare AtomicLongs. Now this is fairly easy, we just need to compare them according to the value they wrap. So here I need to pass a comparator as a parameter, comparingBy, I have to provide a function that takes an AtomicLong and returns a long, which is just the get on that AtomicLong. Okay, so this is the first step of my extraction. Here I have indeed a map of integers and map. entry of actors and AtomicLong being the max according to this AtomicLong. And now I need to extract a further max on this map according to this AtomicLong to have just one map entry here, map. entry of integer and actor and AtomicLong.

Live Coding: Extracting the Max of the Main Map
So once again, I am going to create an entrySet on this, open a stream on this entrySet and take the max providing a comparator and once again call get since this max method is going to return an optional. What is this comparator going to be? Well, I can once again creating using the map. entry comparing my value. What is my value here? It is this entry that is there, so I just need to provide a further comparator here comparing, now I need to compare this object here, which is just an entry, entry. getValue, and open the AtomicLong to compare using this long. Now I should have an object of this type, which is exactly the object I need. So let us create this object. Indeed this is an entry of integer, which is the releaseYear, and the value is itself an entry of actor and AtomicLong. Let us just print out the result (Typing) and now we can see that according to our database, the most seen actor was Phil Hawn played in 24 movies in 1999. So this last example is, of course, quite complex, but you can see that with a little habit and a little mastering of the stream API and the collector API, you can really build very powerful and very efficient data processing pipeline in Java 8.

Live Coding Session Summary
So what did we see in this live coding session? Well we saw how to build a complex data processing stream and indeed this example was a complex one. We could build maps. We extracted their values, their values where themselves are the maps, we merged them. We built streams on them. We extracted max from this streams, et cetera. This was quite complex to set up and quite complex to follow. We could solve a nontrivial and computationaly intensive case, remember we had 170, 000 actors processing in that stream and it took less than a second to extract the maximum. So this API, this way of processing data, is extremely efficient and brings great performances. We didn't use parallelism here. We could have used it to further improve the performance. We built a complex custom collector to finely tune our computation and to compute exactly what we needed and this is extremely powerful. We have ready to use collectors in the JDK, but in case those collectors do not exactly fit our needs, our application needs, then we have the possibility to build our own collectors, even if they are very complex. The Collector API has really been made for that. We use the standard collectors for the simple cases, but in case you need it, you can also build your own collectors.

Other Custom Collectors: The summaryStatistics Pattern
Now this pattern of building custom collectors is used in the JDK itself. Remember when we saw the streams of numbers, the specialized streams of numbers? I told you that there was this method, summaryStatistics, and I told you that we were to see it again in the collectors part and this is the time now to analyze this method. How is this method written? Well it is written like that. And you can see that it calls a collect method. It does not call the collectors. of static method for some reason, but you can see that the three parameters of this collect call are basically a supplier, an accumulator, and a combiner. So this is the exact same pattern as the custom collectors pattern, as the collector. of pattern that we can see here. Now how those two elements have been implemented, we can have a quick look at that. The first method, accept, is in fact very basic. I just compute the sum, the min, and the max in its most classic way, and I just count how many values I have seen. And then I have this combine method that takes another IntSummaryStatistics object and that will merge the content of this other object to the IntSummaryStatistics I am in. So in fact, this IntSummaryStatistics object is my own mutable container and I am exactly in the pattern we saw with the collector. of method.

Module Wrap-up
Now it's time to wrap up this last module of this course. What have we seen in this module? We saw that the collector pattern is the right pattern to compute complex reduction. If you have simple reductions to conduct, well just use the simple pattern and the simple reduce method. But if you have complex reduction to compute, then the collector pattern is the right tool for you. We saw how to collect data in mutable containers and we saw many search mutable containers, strings, collections, maps, this summary statistics object. We could build our own collectors if the standard ones are not enough and this leads to extremely powerful data processing pipeline. We have the collector. of pattern, which is this static method of the collector interface and in fact, I do not need to implement this collector interface itself. In fact, I didn't even show you this collector interface just because we do not need to know how it is built. We saw how to set up all these on a real complex case, the Movies and Actors example. We could combine complex collectors with complex structures, maps of maps, maps of lists, et cetera. And we could add complex post processings, maximum extraction, sorting of streams, et cetera. This is the end of this module and the end of this course. Thank you for watching. I hope you've found it interesting. I hope you could learn things by watching this course. Do not hesitate to post your comments and questions, I will be very happy to answer them. Thank you for your attention.
