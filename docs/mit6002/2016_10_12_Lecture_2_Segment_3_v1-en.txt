...
We ended the last segment looking
at the conditions needed for dynamic programming to work
and asking the obvious question, did they
apply to the knapsack problem?
You'll recall that there were two conditions needed
for dynamic programming to work, optimal substructure
and overlapping subproblems.
When we looked at them before in the context of a small problem,
we basically built a search tree and found,
by looking at the search tree, that we
can show both optimal substructure and overlapping
problems.
So let's go look at the search tree we looked at earlier.
Here is our search tree for choosing a menu.
So there's clearly optimal substructure.
Each parent node combines the solutions
reached by its children to derive
an optimal solution for the subtree rooted at that parent.
So we could start at the top, for example,
and say the right answer to what to do here
is going to be the better of the two answers here and here.
No problem.
We have optimal substructure.
But what about overlapping subproblems?
If we look at what we have here, we'll
see that each node is solving a different problem.
No two nodes have the same contents
of the knapsack and the same two food items to choose from.
So indeed, we could run a dynamic programming algorithm
on this, but it wouldn't give us any speedup,
it wouldn't provide any help with this menu.
So let's look at a slightly different menu
and ask the question for that.
So here, we have a different menu.
Notice that we have two beers, rather than one to choose from.
And so we can take one beer, or we cannot take that beer.
But when we get to the bottom of the tree here,
we have two nodes that are identical.

So indeed, for this menu, we do have nodes that
are solving the same problem.
It doesn't matter if we took the beer in the left or the beer
in the right, we still have a knapsack containing one beer
and have to decide what to do about the pizza and the burger.
Is it necessary to have a menu with multiple copies
of the same item for dynamic programming to work?
The answer is no.
Here's a very simple menu with four items
with the colorful names of A, B, C, and D.
Each of them are different, but let's look
at a search tree for this menu.
The root of the tree, node 0-- I've
numbered the nodes in the order in which they are generated
with a depth-first left-first search-- has a label
indicating that no items have been taken, an empty knapsack.
All of the items remain to be considered, A, B, C, and D.
The number of items that's taken is 0.
And I just assume that 5 is the weight of our knapsack.
So before we've done anything, that's what's available.
In order to make this fit on the screen,
I've actually pruned this tree, so
that we don't see any of the nodes that
are out of the question because the knapsack is already full.
So now, let's look at node 1.
Node 1 indicates that item A has been taken.
B, C, and D remain to be considered.
The value of the items taken is 6,
because that was the value of A. And the knapsack still
has room for another two pounds.
Notice that node 1 has no left child,
because we need to consider item B next,
which weighs three pounds, so it wouldn't fit in the knapsack,
so we pruned it.
And then we can go down, and you can
imagine how all these other items are generated.
So are there overlapping subproblems?
Let's think about what problem is being solved at each node.
The problem is to find the optimal items to take in
from those left to consider, given the remaining
available weight.
Now, what I want you to notice is that the available weight
depends upon the total weight of the items taken,
but not on which items are taken,
or the total value of the items.
So the set of previously chosen items,
or even the value of that set, doesn't matter.
Only its weight matters.
So now, let's look and ask, do we
have overlapping subproblems?
Well, if we look at node 2 and node 7, what we'll see
is, even though a different item has been taken-- A in one case,
and B in another-- we still have the items C and D to consider,
and we still have two pounds left in the knapsack.
So in fact, as far as we're concerned,
they're the same problem.
So indeed, we do have overlapping subproblems.
Now that we know that dynamic programming can
be useful in solving a knapsack problem, let's go implement it.

We'll do that by going to our old max val, which
I hope you remember from the previous segment,
and add a memo to it.
So we're adding the memo as the third argument.
So again, it's to consider avail and the memo,
which has a default value of the empty dictionary.
The key of this dictionary is a tuple
of the items left to be considered
in the available weight because, as we've just seen,
those are the only things that matter in defining
the problem still to solve.
We're going to represent the items left to be considered,
using a trick, which is the length of to consider.
This is a compact way of representing these items.
It works because the items are always removed
from the front of the list.
And so, if we know how long the remaining list is,
we know which items are left.
The first thing that the body of the function does,
the new implementation of max val,
which we called fast max val, is check
whether the optimal choice of items,
given the available weight, is already in the memo.
If it does, it's just going to look it up, of course.
The last thing it does is update the memo.
So this is exactly what we saw in our previous view
of dynamic programming.
All right.
Let's go through the code quickly over here.

So this was the original max val.
We'll come down here and look at fast max val.
It's exactly what we talked about.
We're going to see if the items and avail is in the memo.
If so, we'll just look it up and return it.
Otherwise, we'll see whether there's
nothing left to consider, or no available weight.
In which case, we'll return the result 0 and the empty tuple.
Otherwise, we're going to explore the right branch,
the left branch.
This is exactly the code we saw with max val before.
And then, at the very end, is another difference.
Before we return, we're going to update the memo.
I have rewritten the program, test max val,
from what we've seen before.
And the change I've made is I have this parameter algorithm,
which will allow us to use the same testing
function to test different algorithms for solving
the knapsack problem.
And just to remind you how slow it was originally, what
we're going to do is call it.
Let me make sure I have nothing else about to run here.
No, I'm good.
What I'm going to do is call it with a different number
of items, ranging from 5 to 50.
I'm then going to build a large menu of that size.
We saw this in the last segment.
And then, we'll test it.
And the first time I'm going to test it,
I'm going to test it on the old, slow max val.
Let's see what happens when we run it.

Clomping along pretty quickly through 30 items.
Well, maybe through 25.
Seems to be slowing down for 30, 35.

It does seem to be bogging down here.

Maybe we'll run out at 40.
Got through 35.
Well, we're not going to wait for 40.
Let's kill it, and try fast max val.
So I'll Control C out of this.

We'll come over here and replace the algorithm by the fast one.

So now, you can see why I changed test max val,
so this would be easy to do.
Let's see what we get when we run it.
Wow.
Pretty darn fast.
It went through, and it got through--
didn't even slow down.
Before we could look, it was at 50.
So huge improvement.
We're seeing the dynamic programming
is really a big win here.
Let's try 512, see what happens there.

A little slower, but still pretty darn good.
All right.
Continue with powers of twos.
I'll be ambitious.
We'll see what we get with 1,024.
Charging right along.
Taking a bit longer.

Oh.

Whoops.
Ah.
We have an error message.
Down at the bottom, recursion error.
Down here, maximum recursion depth exceeded in comparison.
What happened?
Well, as we saw in 600.1x, every time a recursive call is made,
Python saves information about the function making the call
on the call stack.
Because of this, the runtime system
places a limit on the depth of recursion.
We can see what this limit is by first
importing the standard Python library module, sys.
So let's come over here and do that.
Import sys, sys, for system.
And now, if we enter sys.getRecursionLimit,
it will tell us that it set the recursion
limit to a depth of 1,000.
Well, that's probably why it failed on 24.
We can also change it, though, so let's do
that-- sys.setRecursionLimit.

I'll set it to 2,000.
Now, let's see what happens if we try and run this.

I hope you're dying of suspense here.

Well, it got through it.
Took a little bit of time, but still, it's
pretty impressive, considering it took much more time
than this than to manage a menu of size 35
without dynamic programming.
Really good.
Just for fun, I wrote an instrumented version of max val
that keeps track of the number of times it's called.
When I ran it, I got the numbers we see in this table.
So the middle column here corresponds roughly
to the number of calls that have been required
without dynamic programming.
Since we know that, without the dynamic programming,
the algorithm is exponential, what we see here
is just exponential growth, which is really fast.
The right column is the actual number
of calls we got when we ran fast max val.
Now, this growth is a little bit hard to quantify for ways
we'll talk about shortly, but it's clearly an enormously less
than exponential.
What's going on?
How can this be?
As I said repeatedly, the problem
is inherently exponential.
Has dynamic programming overturned
the laws of the universe?
Maybe it's simply a miracle of the sort pictured here.
Well, it's not a miracle.
What we've discovered, actually, is
that computational complexity can be a very subtle notion.
The running time of fast max val is
governed by the number of distinct pairs, to consider,
and avail, that is to say how many different keys could we
have in the menu.
The number of possible values of to consider is pretty simple.
It's bounded by the length of the items.
The possible values of avail, it is a bit harder
to characterize.
It's bounded by the number of distinct weights.
How many different ways can you combine
the weights of the available items
to get a different answer.
Let's say, how many different possibilities
are there of available weights left?
This is covered in considerably more detail in the reading
where we talk about pseudo polynomial algorithms, which
is exactly what this is.
It's an algorithm that, most of the time,
runs in polynomial time, in fact, a fairly low order
polynomial.
But worst case, when there are not overlapping subproblems,
as we saw in the first menu we looked at,
it reverts to exponential time.
Let's back up as we finish this lecture
and summarize what we've covered in the first two
lectures of the course.
The first and probably most important thing to say
is that many problems of practical importance
can be formulated as an optimization problem.
Given a problem, ask yourself, can I think about this
as defining an objective function that
has to be minimized or maximized,
subject to some set of constraints, possibly empty?
Once you've formulated the problem,
then probably the first question to ask is,
can I just solve it with a greedy algorithm?
We know that they don't necessarily
provide optimal solutions, because they're optimizing
locally, rather than globally.
But for some problems, they are optimal.
And even when they're not, they often
provide solutions that are perfectly adequate.
When that kind of solution is not,
probably, for most optimization problems,
you're stuck with something that's exponentially hard.
That's really bad.
We can't really run programs that
have exponential running time.
But the good news, as we've just seen,
is that dynamic programming often
yields really good performance for many optimization problems,
specifically those with optimal substructure and overlapping
subproblems.
