...
We ended the last segment looking at a search tree
algorithm for solving the 0/1 knapsack problem.
We saw that, that algorithm gave us
a better answer than any of the greedy algorithms.
Finished quickly.
But we observed that two to the eighth is not a large number.
And said, it was important to look
at what happens when we have a more extensive menu
to choose from.
Now, I'm way too lazy to create a big menu by hand,
so I wrote some code to do that.
The code starts by importing a standard Python
library, called random.
We'll see lots more about random later in the course.
But for now, we'll use only one function from that library.
The function is random.randint.
What that function does is it takes in as its argument
a sequence of integers.
In this case, a sequence from 1 to max val.
And returns a random element from that sequence.
So what you're seeing I'm doing is
I'm selecting random values of how valuable the food is
from 1 to whatever the max val.
And a random value for cost.
From 1 to whatever the max cost is.
We'll then explore different menu sizes, so 5, 10, 15, 20,
all the way up to a menu with 45 items in it.
We'll build the large menus and we'll call test max val.
Let's see what happens when we do that over here in the code.

It starts out pretty zippy.
But as you can see, it slows down quickly.
How quickly?
Well, exponentially quickly, of course.
We'll let this keep running and return to our slides.
So we should ask, if it's exponentially slowing down,
is it hopeless?
Well, in theory, yes.
But in practice, no.
Because dynamic programming can come to our rescue.
Now, you might wonder what dynamic programming is.
I'm sure you wonder.
Don't be fooled into thinking that you can make
a good guess from the name.
After all, a name is sometimes just a name.
And in this case, it's a name that's intentionally deceptive.
I love this quote from its inventor, Richard Bellman,
who was at the time working for the Rand Institute
and being paid by a contract from the Air Force
and the Department of Defense.
Richard Bellman said, "the 1950s were not good years
for mathematical research.
I felt I had to do something to shield Wilson."
Who by the way, was Secretary of Defense at the time.
"And the Air Force from the fact that I was really
doing mathematics.
What title, what name could I choose?"
"It's impossible to use the word dynamic in a pejorative sense.
Try thinking of some combination that
will possibly give it a pejorative meaning,
it's impossible.
Thus, I thought dynamic programming was a good name.
It was something not even a congressman could object to.
So I used it as an umbrella for all my activities."
Of course, that was the 1950s and perhaps,
US congressmen were not quite so objectionable
as they are today.
But I guess that's a topic for a different course.
All right.
What is dynamic programming really?
Before explaining how it works, I
want to look at a simple example where we can use it.
And the simplest example I could think of
was the Fibonacci sequence.
So here's a straightforward recursive implementation
of Fibonacci.
It's clearly correct.
It's terribly inefficient.
If you want to try for example, running it on fib 120,
but don't wait for it to complete.
The complexity of this implementation
is actually a bit hard to derive,
but it's roughly order fib of n.
That is the growth is proportional to the growth
in the value of the result. And the growth rate
of the Fibonacci sequence is substantial.
For example, what's fib 120?
It's that number.
Well, that's a pretty big number.
Is it a prohibitively big number for running fib?
Well, if each recursive call took a nanosecond,
fib 120 would take about 250,000 years to finish.
Which would certainly try the patience of most people.
Let's try and figure out why this simple implementation
takes so long.
Given the tiny amount of code, it's
clear that the problem must be the number of times
fib calls itself.
To understand how big that is, we
can look at this tree which I've expand the recursion
for a simple call of fib of 6.
So the way it works is to compute fib of 6,
we first compute fib of 5 and fib of 4.
Take their sum.
To compute fib of 5, we have to compute fib of 4
and fib of 3, et cetera.
What I want you to notice here is
that in this tree of recursion, we
are recruiting the same thing over and over again.
For example, we're computing fib 4 here.
We're computing fib 4 here.
We're computing fib 3 here and here and here.
So we're doing a lot of the same work over and over again.
It doesn't require genius-- which
isn't to say that Bellman wasn't a genius--
to think that it might be a good idea
to record the value returned by the first time we call fib of 4
say.
And then, look it up rather than compute it each time
it is needed.
This is called memoization and is the key idea
behind dynamic programming.
What we're doing is trading time for space.
We create a table to record what we've done.
And then, before computing fib of x,
check if we've already computed it.
If so, we look it up.
If not, we compute it and then, add it to the table.
As I said, this is called memoization
and is an extremely useful idea that we
can use over and over again to make
algorithms run more quickly.
Here's a memoize implementation of the Fibonacci function.
Very simple.
We call it with n and the memo.
And the first time we call it, the memo
will take the default value of the empty dictionary.
Because we haven't computed anything by then.
We enter it.
If n equals 0 or n equals 1, the base case, we return 1.
Otherwise, we try to return the value of n in the memo,
in the dictionary.
If it's there, we return it.
If it's not there, we'll get a key error
and then we'll compute the value.
And once we get it, store it in the memo,
so we never have to computer to get in.
Let's see what happens when we try and run it.
OK let's return back to our code window
and compare this fast Fibonacci to the straightforward
Fibonacci we looked at before.
I should observe by the way, now that we're back
in the code window-- you'll note we still
have not solved the problem of a menu with 45 items.
So I'm going to give up.
Just Control C out of the console.
All right
So first, let's look and try and compute fib
for all the numbers in range 121.
So again, it starts going pretty quickly
and then it starts slowing down.
You know we're not going to get to 120.
In fact, we may not even get to 40
or maybe not even get to 35 before we-- oh.
Got to 35.
But I'm kind of running out of patience.
You can see it's slowing down and you
can imagine that it's never going
to get to a very big number.
It's probably never going to get to 40 even given our patience.
So I'll quit it.

And I'll comment this out, because I
don't want to run that again.

And now, we'll try the fast fib we just
looked at on the same values.

Wow.

Got to 120 almost instantaneously.
This by the way, is how I knew what value the 120 was,
because I could actually run it.
It's amazing, right?
We went from couldn't even do 36 to got
to 120 in a blink of an eye.
OK.
So we see that dynamic programming,
at least for Fibonacci, is an enormous win.
Well, that doesn't mean it will be
an enormous win and everything.
One of the nice things Bellman did
for us is tell us when we can use dynamic programming, when
it will work well.
And it works well for problems that
exhibit two characteristics, optimal stroke substructure
and overlapping sub problems.
So something is optimal substructure
when a globally optimal solution can
be found by combining optimal solutions to local sub
problems.
So for Fibonacci, we solve fib of x
by combining fib of x minus 1 and fib of x minus 2.
We've already looked at other problems of these properties.
For example, merge sort explains the fact
that a list can be sorted by first sorting the sub lists
and then merging the solutions.
A problem is overlapping sub problems
if an optimal solution involves solving the same problem
multiple times.
We saw that Fibonacci has this property.
Merge sort, by the way, doesn't.
Even though we are performing a merge many times,
we are merging different lists each time.
So what about the 0/1 knapsack problem?
Do these two conditions holds for it?
Can we use dynamic programming to solve the 0/1 knapsack
problem?
You'll have to wait to the next segment to find out.
